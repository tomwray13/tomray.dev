---
title: 'Project 0 - NestJS Starter'
h1: Project 0 - NestJS Starter
date: '2023-07-23'
lastmod: '2023-07-23'
draft: false
summary: Project 0 - NestJS Starter
images: ['/static/images/nestjs-cheat-sheet/banner.png']
isHiddenFromSearch: false
---

Please make sure you've installed the pre-requisites before starting!

## Intro

In this first project, we're going to build a NestJS starter repo which will act as a foundation for all other projects in the course.

We're actually going to build 2 NestJS starter projects:

1. Standard NestJS Starter (for projects with just a single NestJS server)
2. Monorepo NestJS Starter (for projects with multiple apps, e.g. a NestJS server & NextJS server)

Then at the start of each project, we'll `git clone` the most relevant starter project so we can skip a bunch of admin and setup each time we start a project.

The starter repos will also serve as excellent NestJS starter repos for any future projects you start outside of the course!

Here's what's included in the starter repos:

- Adding to the tsconfig.json file
- Setting up the ConfigModule and environment variables for Jest
- Enforcing consistent HTTP response structure
- Configuring some basic HTTP security
- Adding whitelisted validation to the NestJS server
- Setting up NestJS logging
- Docker compose set up for a Postgres database & Redis
- Prisma setup (the ORM we'll be using in each project to interact with the database)
- Redis and CacheService setup

- Jest config (including env variables)
- Setting up a CI Pipeline using Github Actions

Here is the Github repo if you'd like to check out the finished starter, but I do recommend creating your own starter by following this project.

## Using the Nest CLI to spin up the project

The first step is to use the Nest CLI to spin up a new project.

```shell
npx nest new nestjs-starter --package-manager=pnpm --strict
```

We passed in a couple of options worth noting:

`--package-manager=pnpm` - setting the package manager to use pnpm
`--strict` - by default when NestJS scaffolds projects it doesn't use strict mode in TypeScript

With your project now set up, let's proceed!

## Adding to the tsconfig.json file

It's worth adding a couple of extra configs to the `tsconfig.json` file to help you write better code:

`noUnusedLocals`: Report errors on unused local variables (e.g. you declate a variable with `const` and then never user it)

`noUncheckedIndexedAccess`: Prevents runtime errors caused by unexpected undefined values. This one's best explained by an example:

```ts
// define an array
const numbers = [1, 2, 3]

// access array with index 3
const example = numbers[3]

// this will cause a runtime error because index 3 does not exist!
console.log(example.toFixed(2))
return 'Hello World!'
```

By switching on `noUncheckedIndexedAccess`, you will be forced (you'll see the red squigly lines in your IDE) to handle the `undefined` scenario which will help prevent runtime errors. Set this in your tsconfig and thank yourself later ðŸ˜‰.

You can add these to your tsconfig file like this:

```json:tsconfig.json
{
  "compilerOptions": {
    // other config options...
    "noUnusedLocals": true,
    "noUncheckedIndexedAccess": true
  }
}
```

## Add the Config Module configuration

In a classic NodeJS project, you'd need to install the [dotenv](https://www.npmjs.com/package/dotenv) package to use environment variables.

NestJS comes with a built-in config module (that uses the dotenv package under the hood) that you can use to read environment variables.

Before installing it, it's worth first adding to your `.gitignore` file a line for `env`. I'm not sure why Nest doesn't include this in their default scaffold, but it's necessary otherwise your secret variables will be public.

Your `.gitignore` file should now look like:

```.gitignore
# ...

# env
.env
```

Okay, with that set up let's install NestJS Config

```bash
pnpm add @nestjs/config
```

You might see an error like this after installing the package:

```
â€‰WARNâ€‰ Issues with peer dependencies found
.
â””â”€â”¬ ts-loader 9.4.4
  â””â”€â”€ âœ• missing peer webpack@^5.0.0
Peer dependencies that should be installed:
  webpack@^5.0.0
```

This is because NestJS Config has a peer dependency on Webpack 5. We're not using Webpack in this project, so we can safely ignore this warning.

```shell
pnpm add webpack@^5.0.0
```

With the package installed, we can now use the config module.

We're going to add a `/core` directory and respective module which will be a place where we can add services that can be used across the entire application.

You can think of the `/core` directory as a place to add services that are used across the entire application, such as a logger service, a config service, a cache service, etc.

After adding the `/core` directory, add a new file called `core.module.ts` and include the `ConfigModule` along with the `forRoot()` static method:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';

@Module({
  imports: [ConfigModule.forRoot(),],
  providers: [],
  exports: [],
})
export class CoreModule {}
```

Also make sure you import this module into the main `app.module.ts` module:

```ts:app.module.ts
import { Module } from '@nestjs/common';
import { CoreModule } from './core/core.module';

@Module({
  imports: [CoreModule],
  // ...
})
export class AppModule {}

```

**Please note:** As you add more imports to the `CoreModule`, always keep the `ConfigModule` as the first import. Also make sure the `CoreModule` is the first import inside the `AppModule`. Otherwise the other imports won't have access to the environment variables.

Assuming you're using the default `.env` file in your project, you'll now have access to your environment variables by using `process.env` anywhere in your NestJS app. While this approach works, it doesn't offer any type safety.

Instead of using `process.env` in your NestJS app whenever you need to access an environment variable, you can instead use the NestJS ConfigModule we just set up.

[Laravel](https://laravel.com/docs/9.x/configuration) uses a very similar approach where you have custom configuration files inside a `config` directory which point to environment variables.

Let's add a configuration file inside the config directory and add the following::

```typescript:config/configuration.ts
export default () => ({
  environment: process.env.NODE_ENV || `development`,
});
```

You will need to import this configuration file into the `ConfigModule` by using the `load` property:

```typescript:app.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from './config/configuration';

@Module({
  imports: [
    ConfigModule.forRoot({
      load: [config]
    })
  ],
  providers: [],
  exports: [],
})
export class CoreModule {}

```

To now use the values set in the configuration file in one of the modules in our NestJS app, we'd need to import the `ConfigModule` (just like you would with any provider):

```typescript:feature.module.ts
@Module({
  imports: [ConfigModule],
  // ...
})
```

That being said, I prefer to set the `isGlobal` property to true in the `ConfigModule` in `app.module.ts` so that it's available everywhere in the app (and I don't need to import the Config Module everytime).

```typescript:app.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from './config/configuration';

@Module({
  imports: [
    ConfigModule.forRoot({
      isGlobal: true,
      load: [config]
    })
  ],
  providers: [],
  exports: [],
})
export class CoreModule {}

```

We won't go into any specific examples of how to use the ConfigModule inside services as that's covered plenty in the projects ahead! In the meantime, if you want to check out an example of how the ConfigService is injected into a service, check out [this tutorial](https://www.tomray.dev/nestjs-config#using-custom-configuration-files).

## Enforcing consistent HTTP response structure

By wrapping all HTTP responses in a standard structure, you make it easier for clients consuming the API to know what to expect. They can rely on the response always having the same shape, regardless of the specific endpoint.

This can be achieved in NestJS using an interceptor. Interceptors in NestJS allow you to execute logic before or after the main handler (such as a controller method) is called, making them a perfect fit for this use case.

Inside the `/core` directory, add a new directory called `/interceptors` and create a new file called `transform-response.interceptor.ts`:

```ts:transform-response.interceptor.ts
import { Injectable, NestInterceptor, ExecutionContext, CallHandler } from '@nestjs/common';
import { Observable } from 'rxjs';
import { map } from 'rxjs/operators';

@Injectable()
export class TransformResponseInterceptor implements NestInterceptor {
  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {
    return next.handle().pipe(
      map(data => ({ data })),
    );
  }
}
```

What's this code doing?

It's intercepting the response from the controller and wrapping it in an object with a `data` property. This means that the response will always have the same shape, regardless of the specific endpoint.

Specifically, here's more detail breaking down the code:

- Class Definition `TransformResponseInterceptor {}`: This class is going to define our custom interceptor. It implements the NestJS `NestInterceptor` interface, which requires us to implement a single method called `intercept()`.
- Method `intercept()`: This is where the action happens. It receives the current context, which has information about the current request, and next, which lets us handle the request and response.
- Handling the Response `next.handle()`: Here, we are saying, "Go on and handle the request as you normally would, and let me know when you have a response ready."
- Using `pipe` with `map`: This part is where we are telling NestJS what to do with the response once it's ready.
- `pipe`: Think of this as a way to funnel the response through a series of transformations.
- `map`: This is the transformation we want to apply. We're saying, "Take the response and wrap it inside an object with a single property called data."

Why bother with doing this?

- **Consistency**: This interceptor provides a simple but powerful way to ensure that your API has a consistent and well-defined interface.
- **Ease of Extension**: If you decide later that you want to include additional metadata in every response (such as headers for rate limiting or custom diagnostic information), you can do so in the interceptor without having to modify every individual controller.
- **Separation of Concerns**: By handling the response transformation in an interceptor, you keep this logic separate from your actual business logic in the controllers. This can make the code easier to understand, test, and maintain.

One final tweak to this interceptor is to consider pagination. If you have an endpoint that returns a list of items, you might want to include pagination information in the response. For example, you might want to include the total number of items, the current page, and the number of items per page.

Instead of nesting this inside the `data` property, it's better to include it as a sibling property. This is because the `data` property is intended to contain the actual data being returned by the endpoint, and including pagination information inside it would break that convention.

Here's how we can update the interceptor to handle this:

```ts:transform-response.interceptor.ts
@Injectable()
export class TransformResponseInterceptor implements NestInterceptor {
  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {
    return next.handle().pipe(
      map((response) => {
        if (response.data && response.meta) {
          return {
            data: response.data,
            meta: response.meta,
          };
        }
        return { data: response };
      }),
    );
  }
}
```

To implement this interceptor, inside your `core.module.ts` file, add the following:

```typescript:app.module.ts
import { Module } from '@nestjs/common';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { ConfigModule } from '@nestjs/config';
import config from './config/configuration';
import { TransformResponseInterceptor } from '../core/interceptors/transform-response.interceptor';

@Module({
  imports: [
    ConfigModule.forRoot({
      isGlobal: true,
      load: [config]
    })
  ],
  providers: [
    {
      provide: APP_INTERCEPTOR,
      useClass: TransformResponseInterceptor,
    }
  ],
  exports: [],
})
export class CoreModule {}

```

Test an API request and the response should now be wrapped in an object with a `data` property.

It's also worth updating the built in e2e test that Nest created for you to reflect the new data property:

```typescript:app.e2e-spec.ts
import { Test, TestingModule } from '@nestjs/testing';
import { INestApplication } from '@nestjs/common';
import * as request from 'supertest';
import { AppModule } from './../src/app.module';

describe('AppController (e2e)', () => {
  let app: INestApplication;

  beforeEach(async () => {
    const moduleFixture: TestingModule = await Test.createTestingModule({
      imports: [AppModule],
    }).compile();

    app = moduleFixture.createNestApplication();
    await app.init();
  });

  it('/ (GET)', () => {
    return request(app.getHttpServer())
      .get('/')
      .expect(200)
      .expect({ data: 'Hello World!' }); // Updated to reflect new data property
  });
});
```

We'll get into more detail on e2e testing in a section below!

## ðŸ”’ Add HTTP header security

Out of the box, NestJS uses the default HTTP headers which can open you up to security vulnerabilities.

Thankfully, that's why [Helmet](https://helmetjs.github.io/) exists - a widely adopted and well maintained security package which provides HTTP header security out of the box to help prevent common exposures.

It's also super easy to install in a NestJS app too:

```shell
pnpm add helmet
```

And then apply the package as global middleware:

```ts:main.ts
import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import helmet from 'helmet';

async function bootstrap() {
  const app = await NestFactory.create(AppModule);
  app.use(helmet());
  await app.listen(3000);
}
bootstrap();

```

And that's it!

## Setting up a global validation pipe

We'll get into setting up many different example controller routes in the projects ahead.

But one thing is for sure - we'll need to set up validation across these routes to help maintain data integrity and added layers of security.

In NestJS, the standard way to do this is by using DTOs (Data Transfer Objects) in combination with the `class-validator` package.

For example, let's say you have an endpoint to create a user:

```ts
//... rest of the controller file

@Post()
create(@Body() createUserDto: CreateUserDto) {
  return 'This endpoint adds a new user';
}
```

The `CreateUserDto` could be:

```ts
import { IsEmail, IsNotEmpty } from 'class-validator'

export class CreateUserDto {
  @IsEmail()
  email: string

  @IsNotEmpty()
  password: string
}
```

With this in place, if the incoming client payload does not pass the validation rules defined in the DTO, it will automatically response with a `400` with the following response. For example, if an invalid email is passed in:

```json
{
  "statusCode": 400,
  "error": "Bad Request",
  "message": ["email must be an email"]
}
```

This setup, however, is NOT supported out of the box in a NestJS app, so we need to set up the foundations to support this in our NestJS starter.

First install the respective packages:

```bash
pnpm add class-validator class-transformer @nestjs/mapped-types
```

Here's a brief explanation of each package:

- `class-validator`: used to validate incoming request bodies, query parameters, and more, to ensure they meet the specified criteria before reaching your controller's handler methods.
- `class-transformer`: used to transform incoming request objects into instances of specific DTOs (Data Transfer Objects), ensuring they are manipulated and interacted with according to the defined class structure.
- `@nestjs/mapped-types`: particularly useful when you're working with DTOs that have similar shapes but slightly different requirements for various operations, such as creating and updating a resource. You can derive multiple types from a single base type, keeping your code DRY (Don't Repeat Yourself).

In order for DTOs to be used across any route in the NestJS app, we need to define a global validation pipe.

The ValidationPipe is one of the built-in pipes in NestJS, and it's used to validate incoming client payloads, ensuring they match the expected types and constraints. It uses class-validator and class-transformer (the packages we just installed) under the hood to accomplish this.

Update your `main.ts` file to add the global validation pipe:

```ts:main.ts
import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import { ValidationPipe } from '@nestjs/common';

async function bootstrap() {
  const app = await NestFactory.create(AppModule);
  app.use(helmet());
  app.useGlobalPipes(new ValidationPipe()); // add global validation pipe
  await app.listen(3000);
}
bootstrap();

```

Here's a breakdown of what's happening:

- `new ValidationPipe()`: This creates a new instance of the ValidationPipe class.
- `app.useGlobalPipes()`: This method sets the pipe(s) passed into it as global pipes, which means they will be used across all the controllers and routes in the application.

By using this line, you can enforce validation across the entire application, making sure that data coming into the app meets the expected criteria defined using decorators in DTO (Data Transfer Object) classes.

Finally, it's worth adding the `whitelist` property to the ValidationPipe like this:

```ts:main.ts
import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import { ValidationPipe } from '@nestjs/common';

async function bootstrap() {
  const app = await NestFactory.create(AppModule);
  app.use(helmet());
  app.useGlobalPipes(new ValidationPipe({ whitelist: true })); // add global validation pipe
  await app.listen(3000);
}
bootstrap();

```

If `whitelist` is set to true, the ValidationPipe will strip away properties from the input object that do not have any matching decorators in the corresponding DTO. This can be useful to automatically remove any unwanted or unexpected properties sent by the client, thus ensuring that only expected properties are being processed.

## Setting up logging in NestJS

Logging is an important part of any application. It allows you to see what's happening in your application and debug issues.

Nest has a built in logger you can use, but I prefer to use a dedicated logging module which supports JSON logging which is useful for aggregating logs in a production environment.

This is taken from the [NestJS docs](https://docs.nestjs.com/techniques/logger):

> Nest's built-in logger is used for monitoring Nest system behavior, and can also be useful for basic formatted text logging in your feature modules while in development, but production applications often take advantage of dedicated logging modules like Winston.

Let's now build a dedicated logging module that supports JSON. We're going to use Winston as the logging library, so let's first install the package:

```shell
pnpm add winston
```

Inside the `/core` directory, create a new directory called `logger` and add a file called `logger.service.ts`

```ts:logger.service.ts
import { Injectable, LoggerService } from '@nestjs/common';
import * as winston from 'winston';

@Injectable()
export class Logger implements LoggerService {
  private readonly logger: winston.Logger;

  constructor() {
    this.logger = winston.createLogger({
      level: 'info', // Set your desired default log level
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json(),
      ),
      transports: [
        new winston.transports.Console(),
        // Add other transports like file or cloud-based logging solutions
      ],
    });
  }

  log(message: any, context?: string, meta?: any) {
    this.logger.info(message, {
      context,
      meta,
    });
  }

  error(message: any, trace?: string, context?: string, meta?: any) {
    this.logger.error(message, {
      context,
      trace,
      meta,
    });
  }

  warn(message: any, context?: string, meta?: any) {
    this.logger.warn(message, {
      context,
      meta,
    });
  }

  debug(message: any, context?: string, meta?: any) {
    this.logger.debug(message, {
      context,
      meta,
    });
  }

  verbose(message: any, context?: string, meta?: any) {
    this.logger.verbose(message, {
      context,
      meta,
    });
  }
}

```

You'll notice I've included a `meta` argument in each log which allows you to pass in any extra data to each log if you want to give additional context.

Make sure you import the logger service into the `core.module.ts` file and add it to the `providers` array:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from '../config';
import { TransformResponseInterceptor } from './interceptors/transform-response.interceptor';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { Logger } from './logger/logger.service';
import { Global } from '@nestjs/core/decorators';

@Global() // <-- Add this decorator
@Module({
  imports: [ConfigModule.forRoot({ isGlobal: true, load: [config] })],
  providers: [
    { provide: APP_INTERCEPTOR, useClass: TransformResponseInterceptor },
    Logger,  // <-- Add as provider
  ],
  exports: [Logger], // <-- Add as export
})
export class CoreModule {}


```

You'll notice I've also added the `Global()` decorator - this is because we want to make all services (right now just the Logger service) inside this module available to the entire application.

You'll also notice in the `error` log method I've included a way to include the stack trace. This could be used like the following:

```ts
try {
  // Some code that might throw an error
} catch (error) {
  const message = 'An unexpected error occurred!'
  // Optionally, you can use the error message itself
  // const message = error.message;

  const trace = error.stack // Get the stack trace of the error
  const context = 'MyService' // Replace with your specific context

  this.logger.error(error, trace, context)
}
```

Let's give the logger a quick test.

Inside any existing service, inject the logger service you just made and do a log.

Here's an example of how to do it in the `app.service.ts` file:

```ts:app.service.ts
import { Injectable } from '@nestjs/common';
import { Logger } from '../core/logger/logger.service';

@Injectable()
export class AppService {
  constructor(private readonly logger: Logger) {}

  getHello(): string {
    this.logger.log('Hello World!');
    return 'Hello World!';
  }
}
```

This should print to your console:

```shell
{
   "context": "MyContext",
   "message": "Hello World!",
   "level": "info",
   "timestamp": "2023-08-10T10:00:00.000Z"
}

```

The JSON formatting of logs is super practicle in production as it allows you to easily aggregate logs in a centralised logging solution so you can query the logs easily and create dashboards from the logs.

However, in local development it's a little difficult to read, so let's tweak the logging service to prettify the logs in local development only.

```ts:logger.service.ts
import { Injectable, LoggerService } from '@nestjs/common';
import * as winston from 'winston';

@Injectable()
export class Logger implements LoggerService {
  private readonly logger: winston.Logger;

  constructor() {
    const { combine, timestamp, printf, colorize, json } = winston.format;

    // Determine if the application is running in development mode
    const isDevelopment = process.env.NODE_ENV === 'development';

    // Choose a format based on the environment
    const logFormat = isDevelopment
      ? combine(
          colorize(),
          timestamp(),
          printf(({ level, message, timestamp, context, meta, trace }) => {
            return `${timestamp} ${level}: [${context}] ${message} ${
              meta ? JSON.stringify(meta) : ''
            } ${trace ? JSON.stringify(trace) : ''}`;
          }),
        )
      : combine(timestamp(), json());

    this.logger = winston.createLogger({
      level: 'info',
      format: logFormat,
      transports: [
        new winston.transports.Console(),
        // Add other transports like file or cloud-based logging solutions
      ],
    });
  }

  // ... the log methods
}

```

Nice! If you try and do a log again, you'll see the logs are now prettified in local development.

It would also be useful to log each HTTP request that hits our NestJS server. We can do this by creating a middleware that logs each request.

Inside the `/logger` directory, create a new file called `logger.middleware.ts`:

```ts:logger.middleware.ts
import { Injectable, NestMiddleware } from '@nestjs/common';
import { Request, Response, NextFunction } from 'express';
import { Logger } from './logger.service';

@Injectable()
export class LoggerMiddleware implements NestMiddleware {
  constructor(private readonly logger: Logger) {}

  use(req: Request, res: Response, next: NextFunction) {
    const start = Date.now();
    const { method, url, headers, query, body } = req;

    res.on('finish', () => {
      const responseTime = Date.now() - start;
      const message = `${method} ${url} ${res.statusCode} ${responseTime}ms`;
      const statusCode = res.statusCode;
      const logData = {
        responseTime,
        method,
        url,
        headers,
        query,
        body,
      };

      if (statusCode >= 500) {
        this.logger.error(message, undefined, `HTTP`, logData);
      } else if (statusCode >= 400) {
        this.logger.warn(message, `HTTP`, logData);
      } else {
        this.logger.log(message, `HTTP`, logData);
      }
    });

    next();
  }
}

```

This code is basically just logging the request and response data for each HTTP request that hits our server. It includes lots of useful stuff, like how long the request took, the request method, the URL, the headers, the query params, and the body.

Now we need to register this middleware in our application, otherwise it won't work.

As we're using the `CoreModule` as a place to define globally applied config and services, let's add the logging middleware there:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from '../config';
import { TransformResponseInterceptor } from './interceptors/transform-response.interceptor';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { Logger } from './logger/logger.service';
import { Global } from '@nestjs/common/decorators';
import { MiddlewareConsumer } from '@nestjs/common/interfaces';
import { LoggerMiddleware } from './logger/logger.middleware';
import { RequestMethod } from '@nestjs/common/enums';

@Global()
@Module({
  imports: [ConfigModule.forRoot({ isGlobal: true, load: [config] })],
  providers: [
    { provide: APP_INTERCEPTOR, useClass: TransformResponseInterceptor },
    Logger,
  ],
  exports: [Logger],
})
export class CoreModule {
  configure(consumer: MiddlewareConsumer) {
    consumer
      .apply(LoggerMiddleware)
      .forRoutes({ path: '*', method: RequestMethod.ALL });
  }
}


```

Now fire off a HTTP request into your NestJS app and see the logs in your console! Feel free to edit the message in the log to whatever you want in the LoggerMiddleware.

One thing to keep in mind is when we run e2e tests (covered later on), we won't want these HTTP logs to show in all the tests as it will make the test output messy. Let's add a really small tweak to the `LoggerMiddleware` to only log HTTP requests when the `environment` value from the config file is NOT set to `test`:

```ts:logger.middleware.ts
import { Injectable, NestMiddleware } from '@nestjs/common';
import { Request, Response, NextFunction } from 'express';
import { Logger } from './logger.service';
import { ConfigService } from '@nestjs/config';

@Injectable()
export class LoggerMiddleware implements NestMiddleware {
  constructor(
    private readonly logger: Logger,
    private readonly configService: ConfigService,
  ) {}

  use(req: Request, res: Response, next: NextFunction) {
    const environment = this.configService.get(`environment`);
    if (environment !== `test`) { // <-- Only log HTTP requests when not in test mode
      const start = Date.now();
      const { method, url, headers, query, body } = req;

      res.on('finish', () => {
        const responseTime = Date.now() - start;
        const message = `${method} ${url} ${res.statusCode} ${responseTime}ms`;
        const statusCode = res.statusCode;
        const logData = {
          responseTime,
          method,
          url,
          headers,
          query,
          body,
        };

        if (statusCode >= 500) {
          this.logger.error(message, undefined, `HTTP`, logData);
        } else if (statusCode >= 400) {
          this.logger.warn(message, `HTTP`, logData);
        } else {
          this.logger.warn(message, `HTTP`, logData);
        }
      });
    }
    next();
  }
}

```

Nice! Now we have a custom logger service that we can use throughout our NestJS app. We also have a middleware that logs each HTTP request that hits our server (except for when we're running tests).

You may have notice some of the automated NestJS logs:

- When the Nest app starts
- When there is a missing dependency
- When an error bubbles up to the NestJS's built in exception filter

These logs are useful, but they don't use our custom logger service. This would be great to set up so all our logs are consistend. Let's update them to use our custom logger service.

You just need to update the `main.ts` file to use our custom logger service.

```ts:main.ts
import { NestFactory } from '@nestjs/core';
import { AppModule } from './app.module';
import { ValidationPipe } from '@nestjs/common';

async function bootstrap() {
  const app = await NestFactory.create(AppModule, { logger: new Logger() }); // <-- Edit this line
  app.use(helmet());
  app.useGlobalPipes(new ValidationPipe({ whitelist: true }));
  await app.listen(3000);
}
bootstrap();
```

Now the built-in NestJS logs will use our custom logger service.

## Docker Compose for Postgres & Redis

For all the projects in the course we'll be using a Postgres database to store data. We'll also be using Redis for caching.

We're going to use a Docker Compose file to set this up. You'll need Docker installed on your machine to set this up!

Docker Compose is a tool that allows you to spin up 'containers' i.e. local instances of services on your machine that simulate a deployed environment.

Add a `docker-compose.yml` file at the root of your project and add the following:

```yml:docker-compose.yml
version: '3.8'

services:
  nestjs_postgres: # Needs updating
    image: postgres:alpine
    container_name: nestjs_postgres # Needs updating
    restart: always
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data

  nestjs_redis: # Needs updating
    image: redis:alpine
    container_name: nestjs_redis # Needs updating
    ports:
      - '6379:6379'
    volumes:
      - redis_data:/data

networks:
  default:
    name: nestjs_starter  # Needs updating

volumes:
  postgres_data:
  redis_data:

```

I've left a few comments in there as reminders to update those names to something more specific to your project. We'll do this in each project we set up.

In your `.env` file, add the following POSTGRES variables:

```env
NODE_ENV="development"
POSTGRES_USER="username"
POSTGRES_PASSWORD="password"
```

To spin up the Postgres & Redis containers, run the following command:

```bash
docker-compose up -d
```

To run your NestJS server locally, you'll need to run:

```bash
pnpm start:dev
```

It's a little annoying to run 2 commands everytime you need to spin up the local server, so let's add a single script to do both.

Inside the package.json file, add the following script:

```json
"scripts": {
  // other scripts
  "docker:start": "docker-compose up -d",
},
```

And then update the existing `start:dev` script to run both commands:

```json
"scripts": {
  // other scripts
  "start:dev": "pnpm docker:start && nest start --watch",
},
```

Nice! Now you can run `pnpm start:dev` to spin up the local server and the Postgres container.

## Setting up Prisma

In the projects ahead, we're going to use Prisma as the ORM layer to interact with the database.

We'll be covering lots of Prisma in the course, but for now we just need to set up the foundations that can be leveraged by projects later.

In the root of the project, run the following command:

```shell
npx prisma init
```

This does two things:

1. Creates a directory called `prisma` with a file inside called `schema.prisma`
2. Adds an `.env` file to the root of the project

I like to rename the `prisma` directory to `database` and move it inside `src` to keep things tidy.

The directory structure inside the NestJS app looks like this:

```
src
  /database
    schema.prisma
  main.ts
```

Because we've moved removed the Prisma directory and moved the `schema.prisma` file, we just need to add this to our `package.json` file so Prisma knows where to find it:

```json:package.json
{
  ...

  "prisma": {
    "schema": "src/database/schema.prisma"
  }

  ...
}
```

So, what is this `schema.prisma` file?

The `schema.prisma` file is the main configuration file for Prisma. If you open up the file, you'll see the following:

```json
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}
```

The `datasource` provides the connection to your Postgres database by looking up a URL in the ENV file.

This is important - using the env file allows us to have a different Postgres database for our various environments (local, staging, production, etc).

Your local Postgres server already exists (as we set this up in the previous step with Docker Compose) - so we just need to grab these credentials and update our env file.

Before changing anything, your `env` file should look something like this as Prisma auto-generated the `DATABASE_URL` for you:

```env:.env
POSTGRES_USER="username"
POSTGRES_PASSWORD="password"

# This was inserted by `prisma init`:
# Environment variables declared in this file are automatically made available to Prisma.
# See the documentation for more detail: https://pris.ly/d/prisma-schema#accessing-environment-variables-from-the-schema

# Prisma supports the native connection string format for PostgreSQL, MySQL, SQLite, SQL Server, MongoDB and CockroachDB.
# See the documentation for all the connection string options: https://pris.ly/d/connection-strings

DATABASE_URL="postgresql://johndoe:randompassword@localhost:5432/mydb?schema=public"
```

We can remove the auto-generated comments from Prisma and update the `DATABASE_URL` to use our existing environment variables, like so:

```env:.env
POSTGRES_USER="username"
POSTGRES_PASSWORD="password"

DATABASE_URL="postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/mydb?schema=public"
```

In fact, I would take this one step further by abstracting out the other relevant variables:

```env:.env
POSTGRES_HOST="localhost"
POSTGRES_PORT="5432" # Needs to match the port in docker-compose.yml
POSTGRES_NAME="mydb"
POSTGRES_USER="username"
POSTGRES_PASSWORD="password"

DATABASE_URL="postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_NAME}?schema=public"
```

Now your Prisma configuration is using your local Postgres server as the data source.

In order for the NestJS app to get and mutate data from the database we need to use Prisma Client.

```shell
pnpm add @prisma/client
```

Prisma client is a type-safe database client directly tailored to the models you define in the `schema.prisma` file.

As per the [NestJS docs](https://docs.nestjs.com/recipes/prisma#install-and-generate-prisma-client), it's best to abstract the Prisma Client into its own service.

Let's create a new file inside the `database` directory we defined in a previous step called `database.service.ts`:

```shell
touch src/database/database.service.ts
```

And then inside the file, add the following code:

```ts:prisma.service.ts
import { Injectable, OnModuleInit } from '@nestjs/common';
import { PrismaClient } from '@prisma/client';

@Injectable()
export class DatabaseService extends PrismaClient implements OnModuleInit {
  async onModuleInit() {
    await this.$connect();
  }
}
```

To share the Prisma service, we need to create a module that exports the Prisma service.

```shell
touch src/database/database.module.ts
```

In this Prisma module, add the Prisma service as a provider and an export:

```ts:database.module.ts
import { Module } from '@nestjs/common';
import { DatabaseService } from './database.service';

@Module({
  providers: [DatabaseService],
  exports: [DatabaseService],
})
export class DatabaseModule {}


```

With an injectable Prisma service now available to use across our NestJS app, it can now be used by other modules in future projects!

Finally, let's add a few scripts that will make using Prisma easier.

```json:package.json
{
  ...

  "scripts": {
    // other scripts
    "db:migrate:dev": "npx prisma migrate dev", // for running database migrations locally
    "db:migrate:prod": "npx prisma migrate deploy", // used for deployment in CI pipeline
    "db:studio": "npx prisma studio", // a GUI for viewing & mutating database records
  }

  ...
}
```

You'll need to use the `db:migrate:dev` script after each change to the `schema.prisma` file.

The other scripts will be explained in future steps below!

We've not added any database tables as this is very specific for each project - those will get added in the projects ahead.

## Setting up the cache

In an earlier step, we set up a local Redis server using Docker Compose.

Let's now update the code so we can use get and set data to the Redis cache instance.

Let's start by installing the NestJS caching packages:

```shell
pnpm add @nestjs/cache-manager cache-manager
```

We're going to apply the cache set inside the `CoreModule`, so go there and add the `CacheModule`:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from '../config';
import { TransformResponseInterceptor } from './interceptors/transform-response.interceptor';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { Logger } from './logger/logger.service';
import { Global } from '@nestjs/common/decorators';
import { MiddlewareConsumer } from '@nestjs/common/interfaces';
import { LoggerMiddleware } from './logger/logger.middleware';
import { RequestMethod } from '@nestjs/common/enums';
import { CacheModule } from '@nestjs/cache-manager';

@Global()
@Module({
  imports: [
    ConfigModule.forRoot({ isGlobal: true, load: [config] }),
    CacheModule.register() // <- add this line
  ],
  providers: [
    { provide: APP_INTERCEPTOR, useClass: TransformResponseInterceptor },
    Logger,
  ],
  exports: [Logger],
})
export class CoreModule {
  configure(consumer: MiddlewareConsumer) {
    consumer
      .apply(LoggerMiddleware)
      .forRoutes({ path: '*', method: RequestMethod.ALL });
  }
}

```

This sets up the cache manager with the default configuration - you'll now have an in-memory cache!

We want Redis, so let's next install the required packages:

```shell
pnpm add cache-manager-redis-store@2
pnpm add --D @types/cache-manager-redis-store
```

Next up, we need to add some environment variables for Redis.

Let's update the config file with the following:

```typescript:config/configuration.ts
export default () => ({
  environment: process.env.NODE_ENV || `development`,
  redis: {
    host: process.env.REDIS_HOST,
    port: process.env.REDIS_PORT,
    username: process.env.REDIS_USERNAME,
    password: process.env.REDIS_PASSWORD,
  },
});
```

Make sure in your `.env` file you have the following:

```env:.env
REDIS_HOST="localhost"
REDIS_PORT="6379" # Needs to match the port in docker-compose.yml
REDIS_USERNAME=""
REDIS_PASSWORD=""
```

Note how the `REDIS_HOST` and `REDIS_PORT` are the same as what's defined in the `docker-compose.yml` file. Also note how the `REDIS_USERNAME` and `REDIS_PASSWORD` are empty strings - this is because we're using the default Redis configuration. When we get to deploying the projects we'll update these values in each hosting platform.

Let's now update the `CacheModule` to use Redis:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import config from '../config';
import { TransformResponseInterceptor } from './interceptors/transform-response.interceptor';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { Logger } from './logger/logger.service';
import { Global } from '@nestjs/common/decorators';
import { MiddlewareConsumer } from '@nestjs/common/interfaces';
import { LoggerMiddleware } from './logger/logger.middleware';
import { RequestMethod } from '@nestjs/common/enums';
import { CacheModule } from '@nestjs/cache-manager';
import * as redisStore from 'cache-manager-redis-store';

@Global()
@Module({
  imports: [
    ConfigModule.forRoot({ isGlobal: true, load: [config] }),
    CacheModule.registerAsync({
      imports: [ConfigModule],
      useFactory: async (configService: ConfigService) => ({
        isGlobal: true,
        store: redisStore,
        host: configService.get('redis.host'),
        port: configService.get('redis.port'),
        username: configService.get('redis.username'),
        password: configService.get('redis.password'),
        no_ready_check: true,
        ttl: 10,
      }),
      inject: [ConfigService],
    }),
  ],
  providers: [
    { provide: APP_INTERCEPTOR, useClass: TransformResponseInterceptor },
    Logger,
  ],
  exports: [Logger],
})
export class CoreModule {
  configure(consumer: MiddlewareConsumer) {
    consumer
      .apply(LoggerMiddleware)
      .forRoutes({ path: '*', method: RequestMethod.ALL });
  }
}
```

If you now spin up your local server with `pnpm start:dev`, this will connect to the Redis instance and you'll be able to use the cache! You may notice, however, the following console warning:

```
node_redis: Warning: Redis server does not require a password, but a password was supplied
```

This is totally optional but I like to remove this warning by adding the following to the `useFactory` function:

```ts:core.module.ts
@Module({
  imports: [
    // ...
    CacheModule.registerAsync({
      imports: [ConfigModule],
      useFactory: async (configService: ConfigService) => {
        const username = configService.get('redis.username');
        const password = configService.get('redis.password');
        return {
          isGlobal: true,
          store: redisStore,
          host: configService.get('redis.host'),
          port: configService.get('redis.port'),
          ...(username && { username }),
          ...(password && { password }),
          no_ready_check: true,
          ttl: 10,
        };
      },
      inject: [ConfigService],
    })
  ],

  // ...
})
```

Nice! The console warning will now be gone.

You can now inject the `CACHE_MANAGER` token into any service or controller to get access to the cache manager (as shown [here](https://www.tomray.dev/nestjs-caching-redis#injecting-the-cache-manager)).

However, I prefer to encapsulate the caching logic in a `CacheService` and if you ever need to change the caching strategy or underlying cache implementation, you only have to do it in one place.

Inside the `/core` directory, add a `/cache` directory and inside there add a file called `cache.service.ts`:

```ts:cache.service.ts
import { CACHE_MANAGER } from '@nestjs/cache-manager';
import { Injectable, Inject } from '@nestjs/common';
import { Cache } from 'cache-manager';

@Injectable()
export class CacheService {
  constructor(@Inject(CACHE_MANAGER) private readonly cache: Cache) {}

  async get<T>(key: string): Promise<T | undefined> {
    return await this.cache.get<T>(key);
  }

  async set(key: string, value: any, ttl?: number): Promise<void> {
    await this.cache.set(key, value, ttl);
  }

  async del(key: string): Promise<void> {
    await this.cache.del(key);
  }

  async reset(): Promise<void> {
    await this.cache.reset();
  }

  async onModuleDestroy() {
    const redisClient = (this.cache.store as any).getClient();
    redisClient.quit();
  }
}

```

The 4 methods allow us to get, set, delete and reset the cache. The reset method will be used in a later step when we want to clear the cache before each e2e test!

There's also a `onModuleDestroy` method is a built in NestJS lifecycle hook that will be called when the module is destroyed. This will be used to close the Redis connection when the application is shut down. The reason this is required is because [the Redis connection is kept open by default](https://github.com/nestjs/nest/issues/1621) which causes issues when running e2e tests.

Finally, back in the `CoreModule`, add the `CacheService` to the providers and exports:

```ts:core.module.ts
import { Module } from '@nestjs/common';
import { ConfigModule, ConfigService } from '@nestjs/config';
import config from '../config';
import { TransformResponseInterceptor } from './interceptors/transform-response.interceptor';
import { APP_INTERCEPTOR } from '@nestjs/core';
import { Logger } from './logger/logger.service';
import { Global } from '@nestjs/common/decorators';
import { MiddlewareConsumer } from '@nestjs/common/interfaces';
import { LoggerMiddleware } from './logger/logger.middleware';
import { RequestMethod } from '@nestjs/common/enums';
import { CacheService } from './cache/cache.service';
import { CacheModule } from '@nestjs/cache-manager';
import * as redisStore from 'cache-manager-redis-store';

@Global()
@Module({
  imports: [
    ConfigModule.forRoot({ isGlobal: true, load: [config] }),
    CacheModule.registerAsync({
      imports: [ConfigModule],
      useFactory: async (configService: ConfigService) => {
        const username = configService.get('redis.username');
        const password = configService.get('redis.password');
        return {
          isGlobal: true,
          store: redisStore,
          host: configService.get('redis.host'),
          port: configService.get('redis.port'),
          ...(username && { username }),
          ...(password && { password }),
          no_ready_check: true,
          ttl: 10,
        };
      },
      inject: [ConfigService],
    }),
  ],
  providers: [
    { provide: APP_INTERCEPTOR, useClass: TransformResponseInterceptor },
    Logger,
    CacheService,
  ],
  exports: [Logger, CacheService],
})
export class CoreModule {
  configure(consumer: MiddlewareConsumer) {
    consumer
      .apply(LoggerMiddleware)
      .forRoutes({ path: '*', method: RequestMethod.ALL });
  }
}

```

As mentioned above, as the `@Global()` decorator is used, the `CacheService` will be available to all modules.

## Unit test set up

In all of the projects ahead, we will be doing unit testing with Jest.

The purpose of unit testing is to verify a small piece of behavior works.

By adding unit tests, you're investing in your project so that the future version of yourself (or other colleagues working on the project) can operate with speed and confidence - they can add new functionality or do refactoring without worrying about breaking existing functionality.

NestJS comes with a test runner (Jest) out of the box, however, there is 1 package we need to install to help with mocking.

Before installing, let me provide a quick example.

Imagine you have a method which fetches and returns a pokemon:

```ts:pokemon.service.ts

import { Injectable, InternalServerErrorException } from '@nestjs/common';
import { HttpService } from '@nestjs/axios';

@Injectable()
export class PokemonService {
  constructor(private httpService: HttpService) {}

  async getPokemon(id: number) {
    const { data } = await this.httpService.axiosRef({
      url: `https://pokeapi.co/api/v2/pokemon/${id}`,
      method: `GET`,
    });

    if (!data || !data.species || !data.species.name) {
      throw new InternalServerErrorException();
    }

    return data.species.name;
  }
}
```

This method takes in an ID and returns the respective Pokemon.

In unit testing, a 'dependency' refers to a part of the system that your code relies on, like an external service or database. In the given example, the dependency is the HttpService, which is used to make an external API call to fetch the PokÃ©mon details.

When we test a unit of code, we want to isolate it from these external dependencies to ensure that we're only testing the logic of that specific piece of code. This isolation helps in pinpointing where an error might be happening, and it also ensures that the test is fast and reliable. We don't want our test to depend on the availability or behavior of an external service, as this could lead to false negatives or positives in our testing.

That's where mocking comes into play. 'Mocking' is a technique where we replace a dependency with a 'fake' version that we control. This allows us to test our code's reaction to different scenarios without having to actually interact with the real dependency.

Now, this is where the @golevelup/ts-jest package becomes invaluable. It provides utilities that make it easier to create these mocks and integrate them with Jest, the testing framework used by NestJS. By using @golevelup/ts-jest, we can simplify the process of setting up our mocks, writing our tests, and maintaining them as our code evolves.

```shell
pnpm add @golevelup/ts-jest --D
```

We'll go into lots of specific examples of mocking (using this package) in the projects ahead!

## E2E & integration testing set up

In all the projects ahead where relevant, we will do e2e and integration testing with Jest and Supertest.

What is e2e and integration testing and what's the difference between the two?

End-to-End (e2e) testing involves testing the complete flow of an application, from making the API request all the way to the backend processing and responses. Unlike unit testing, which focuses on individual components or functions, e2e testing simulates real-world user behavior and checks how the whole system operates together.

Integration tests focus on testing the interaction between two or more units or components within an application. These can be interactions between different functions, classes, modules, or even external services like databases. The goal is to ensure that these parts work correctly when combined.

So they are quite different.

However, one commonality between e2e and integration tests is that they often involve actual dependencies, such as databases or external services, instead of using mocks or fakes. In our case, that's a Postgres database and Redis cache.

This contrasts with unit testing, where isolating the unit under test is key. In e2e and integration testing, utilizing real dependencies is crucial, as the objective is to test how multiple components work together in a realistic environment. Mocking these dependencies could obscure issues and lead to false assurances, potentially defeating the purpose of these tests.

Therefore, we need to update the built-in Jest config in NestJS with the following:

1. Allow for both e2e tests and integration tests to be run
2. Alongside the local setup, also have instances of both Postgres & Redis dedicated to running e2e and integration tests
3. Update the scripts in package.json to run the tests using the test environment
4. Ensure the state of the database and cache after each individual test is cleaned up so that the next test can run with a clean slate

Let's dive in!

You may have notice that NestJS comes up 2 Jest configs out of the box:

1. `jest-e2e.json` - for e2e & integration tests
2. The Jest config defined in `package.json` - for unit tests

In the `jest-e2e.json` config file, it should look like this:

```json:jest-e2e.json
{
  "moduleFileExtensions": ["js", "json", "ts"],
  "rootDir": ".",
  "testEnvironment": "node",
  "testRegex": ".e2e-spec.ts$",
  "transform": {
    "^.+\\.(t|j)s$": "ts-jest"
  }
}

```

Update the `testRegex` to the following:

```json
{
  "testRegex": "(.e2e-spec.ts$|.int-spec.ts$)"
}
```

This will mean you add the suffix `.e2e-spec.ts` to your e2e tests and `.int-spec.ts` to your integration tests used anywhere across the NestJS app.

So the next thing to do is set up the dedicated testing environment for the Postgres database and Redis cache.

We could use our local development set up, but that would mean our local database and cache state would be affected by the tests which is not helpful or ideal. Therefore, it makes sense to setup a dedicated environment for testing.

Similar to our local development set up, we'll use Docker Compose.

Add a new Docker Compose file at the root of the project called `docker-compose-test.yml`.

Here's how it should look:

```yml:docker-compose-test.yml
version: '3.8'

services:
  nestjs_postgres_test:
    image: postgres:alpine
    container_name: test_postgres
    restart: 'no'
    env_file:
      - .env.test
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - '5444:5432' # Different port to avoid conflict with the development environment
    volumes:
      - test_postgres_data:/var/lib/postgresql/data

  nestjs_redis_test:
    image: redis:alpine
    container_name: test_redis
    ports:
      - '6380:6379' # Different port to avoid conflict with the development environment
    volumes:
      - test_redis_data:/data

networks:
  default:
    name: nestjs_starter_test

volumes:
  nestjs_postgres_test:
  nestjs_redis_test:

```

You'll notice it's very similar to our local development set up, except:

- we've changed the ports and container names to avoid conflicts with the local development environment
- I've changed the env_file directive to point to .env.test, assuming that this file will contain the specific environment variables for your testing setup.
- I've set the restart policy to 'no' for the testing environment. In a development environment, you might want services to restart automatically if they fail, but in a testing environment, a failure is usually something you want to investigate.

Let's add the `.env.test` file which will contain the environment variables for the test environment (note the ports should match what's in the docker compose file we just added):

```env:.env.test
NODE_ENV="test"

POSTGRES_HOST="localhost"
POSTGRES_PORT="5444" # Matches the port in docker-compose-test.yml
POSTGRES_NAME="mydb"
POSTGRES_USER="username"
POSTGRES_PASSWORD="password"

DATABASE_URL="postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_NAME}?schema=public"

REDIS_HOST="localhost"
REDIS_PORT="6380" # Matches the port in docker-compose-test.yml
REDIS_USERNAME=""
REDIS_PASSWORD=""
```

To test that this works, run the following command:

```shell
docker-compose -f docker-compose-test.yml up -d
```

Both services should now be running!

However, we don't want to run them on our local machine (that was just to see if everything was set up okay). We want these to be spun up before our e2e and integration tests so they can be used in them.

Let's now get into setting up the scripts in package.json to do this!

You'll notice in the built-in package.json file, there's already a script for running e2e tests:

```json:package.json
{
  "scripts": {
    "test:e2e": "jest --config ./jest-e2e.json"
  }
}
```

Let's first tweak this to use the `--runInBand` flag provided by Jest:

```json:package.json
{
  "scripts": {
    "test:e2e": "jest --runInBand --config ./jest-e2e.json"
  }
}
```

This flag causes all tests to be run serially in the current process rather than creating a worker pool of child processes to run tests concurrently.

This is important for our test set up because each test may make state changes to the database or cache, and we want to ensure that each test runs in a clean environment.

The other tweak we need to make is to make sure the test environment variables in `.env.test` are used when running the e2e and integration tests, as this is where all the credentials for the test database and cache are stored.

To do this, we need to install a package as a dev dependency called `dotenv-cli`:

```shell
pnpm add dotenv-cli --save-dev
```

The command `dotenv -e env.test --` will load the environment variables from the env.test file before running the Jest tests, so let's update the `test:e2e` script to use this:

```json:package.json
{
  "scripts": {
    "test:e2e": "dotenv -e env.test -- jest --runInBand --config ./jest-e2e.json"
  }
}
```

Nice! One thing I've noticed with this set up is that if your `.env.test` file is missing some environment variables that are used in the tests, the tests will default to the values in the `.env` file. This is not ideal as it could lead to false positives.

So to make sure that when the `test:e2e` script is run it only uses the environment variables in the `.env.test` file, we can leverage Jest's global setup and teardown hooks.

What we're going to do is:

- When the `test:e2e` script is run, before running any of the tests we'll temporarily rename the `.env` file to `.env.temp`
- After all the tests have run, we'll rename the `.env.temp` file back to `.env`

In the `/test` directory, add a new file called `global-setup.ts` with the following code:

```ts
import { existsSync, renameSync } from 'fs'

export default async () => {
  if (existsSync('.env')) {
    renameSync('.env', '.env.temp')
  }
}
```

Also in the `/test` directory, add a new file called `global-teardown.ts` with the following code:

```ts
import { existsSync, renameSync } from 'fs'

export default async () => {
  if (existsSync('.env.temp')) {
    renameSync('.env.temp', '.env')
  }
}
```

And in the `jest-e2e.json` config file, add the following:

```json:jest-e2e.json
{
  // ...
  "globalSetup": "<rootDir>/test/global-setup.ts",
  "globalTeardown": "<rootDir>/test/global-teardown.ts",
}
```

This solution isn't perfect, but does the job of ensuring the tests only use the environment variables in the `.env.test` file. If you have a better solution please let me know and I can update this guide!

The next thing we need to do is setup the scripts to start the Docker services before all the tests are run and stop them after all the tests are run.

Here's the flow of how we want the scripts to execute:

1. Start the Docker services
2. Run the tests
3. Stop the Docker services

Let's start by adding a couple of Docker scripts (to solve point 1 and 4) for handling the starting and stopping of the Docker services:

```json:package.json
{
  "scripts": {
    "docker:start:test": "dotenv -e .env.test -- docker-compose -f docker-compose-test.yml up -d",
    "docker:down:test": "dotenv -e .env.test -- docker-compose -f docker-compose-test.yml down",
  }
}
```

We can now update the testing script to:

```json:package.json
{
  "scripts": {
    "test:e2e": "pnpm docker:start:test && dotenv -e env.test jest --runInBand --config ./jest-e2e.json && pnpm docker:down:test"
  }
}
```

Running `pnpm test:e2e` should now start the Docker services, run the tests, and then stop the Docker services.

In a real-world scenario, databases often take some time to initialize, especially when they have to set up data structures, perform recovery procedures, or load large datasets. If you attempt to run tests or connect to the database before it's ready, you may encounter errors or unexpected behavior.

So, what we need to do is add a database readiness check before running the tests. We can tweak the docker compose test file by adding a healthcheck to the Postgres service:

```yml:docker-compose-test.yml
version: '3.8'

services:
  test_nestjs_postgres:
    image: postgres:alpine
    container_name: test_nestjs_postgres
    restart: 'no'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U $POSTGRES_USER']
      interval: 10s
      timeout: 5s
      retries: 5
      // ... rest of the file
```

This uses the pg_isready command to check the database's availability. It will run every 10 seconds, and if it fails, it will retry 5 times before giving up.

This is exactly what we need!

The next (and final) thing we need to do to finish the setup is to add a script to reset and migrate the database to a clean slate before running the tests.

This ensures that the schema and database structure are in the correct state before the tests run.

Let's add a script to handle this now:

```json:package.json
{
  ...

  "scripts": {
    // other scripts
    "db:migrate:test": "dotenv -e .env.test -- npx prisma migrate reset --force --skip-seed", // used to clear database before each e2e test
  }

  ...
}
```

Let me explain what's happening with this script:

- We're using the `dotenv` command to load the environment variables from the `.env.test` file before running the command. If you look in your `prisma.schema` file, you'll see it references the database URL from the environment variables. Using the `dotenv` command ensures that the correct database URL (from `.env.test`) is used when running the command.
- `npx prisma migrate reset` will reset the database to a clean state. This means that all the data in the database will be deleted, and the schema will be recreated.
- The `--force` flag is used to skip the confirmation prompt and the `--skip-seed` flag is used to skip seeding the database with data. We don't need to seed the database with data because we'll be doing this in the tests.

If you run the command `pnpm db:migrate:test` locally, you'll get the following error:

```shell
You don't have any models defined in your schema.prisma, so nothing will be generated.
You can define a model like this:

model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
}
```

This is because we haven't defined any models in our `prisma.schema` file yet. We'll do this in the projects ahead and update the `pnpm test:e2e` script to include the `pnpm db:migrate:test` script then.

To recap, here's what we've done so far:

1. ~Allow for both e2e tests and integration tests to be run~
2. ~Alongside the local setup, also have instances of both Postgres & Redis dedicated to running e2e and integration tests~
3. ~Update the scripts in package.json to run the tests using the test environment~
4. Ensure the state of the database and cache after each individual test is cleaned up so that the next test can run with a clean slate

So the final part is clearing the database and cache after each individual test is run.

For the cache, we already added a method called `reset()` to the `CacheService` in the previous section. This method will clear the cache.

To clear the database, let's add a method called `resetDb` to the existing `DatabaseService`:

```ts:database.service.ts
import { Injectable, OnModuleInit } from '@nestjs/common';
import { PrismaClient } from '@prisma/client';

@Injectable()
export class DatabaseService extends PrismaClient implements OnModuleInit {
  async onModuleInit() {
    await this.$connect();
  }

  async resetDb() {
    const tablenames: Array<{ tablename: string }> = await this.$queryRaw<
      Array<{ tablename: string }>
    >`SELECT tablename FROM pg_tables WHERE schemaname='public'`;

    const tables = tablenames
      .map(({ tablename }) => tablename)
      .filter((name) => name !== '_prisma_migrations')
      .map((name) => `"public"."${name}"`)
      .join(', ');

    try {
      await this.$executeRawUnsafe(`TRUNCATE TABLE ${tables} CASCADE;`);
    } catch (error) {
      console.log({ error });
    }
  }
}


```

This is [how Prisma recommends](https://www.prisma.io/docs/concepts/components/prisma-client/crud#delete-all-records) to delete all records in all the tables in a database.

Inside the `/test` directory in the `jest-e2e.json` file, add the setupFiles property like this:

```json:jest-e2e.json
{
  "moduleFileExtensions": ["js", "json", "ts"],
  "rootDir": ".",
  "testEnvironment": "node",
  "testRegex": "(.e2e-spec.ts$|.int-spec.ts$)",
  "transform": {
    "^.+\\.(t|j)s$": "ts-jest"
  },
  "setupFilesAfterEnv": ["./setup.ts"]
}

```

Then add a new file called `setup.ts` and add the following code:

```ts:setup.ts
import { INestApplication } from '@nestjs/common';
import { Test, TestingModule } from '@nestjs/testing';
import { AppModule } from '../src/app.module';
import { CacheService } from '../src/core/cache/cache.service';
import { DatabaseService } from '../src/database/database.service';

let app: INestApplication;
let moduleFixture: TestingModule;
let cache: CacheService;
let database: DatabaseService;

beforeAll(async () => {
  moduleFixture = await Test.createTestingModule({
    imports: [AppModule],
  }).compile();

  app = moduleFixture.createNestApplication();
  cache = moduleFixture.get<CacheService>(CacheService);
  database = moduleFixture.get<DatabaseService>(DatabaseService);
  await app.init();
});

afterEach(async () => {
  await database.resetDb();
  await cache.reset();
});

afterAll(async () => {
  await app.close();
});

```

Let's talk through what we're doing here:

- We're using Jest's `beforeAll` hook to create the NestJS application and initialize it. We're also getting the `CacheService` and `DatabaseService` instances from the NestJS application. This will run once before all the tests are run.
- We're using Jest's `afterEach` hook to reset the database and cache after each individual test is run. This ensures that the database and cache are in a clean state before each test is run.
- Finally we're using Jest's `afterAll` hook to close the NestJS application after all the tests are run. This ensures that the NestJS application is closed after all the tests are run and closes the connection to the database.

If you run the test with command `pnpm test:e2e`, you'll get an error:

```shell
Nest could not find DatabaseService element (this provider does not exist in the current context)
```

This is because in this starter repo, we've not imported the `DatabaseModule` anywhere so it's not available in the `AppModule`.

In the projects ahead, we'll be using the `DatabaseModule` a lot, so for now let's just comment out the `resetDb()` method in our tests so we can easliy add it back later on:

```ts:setup.ts
import { HttpServer, INestApplication } from '@nestjs/common';
import { Test, TestingModule } from '@nestjs/testing';
import { AppModule } from '../src/app.module';
import { CacheService } from '../src/core/cache/cache.service';
// import { DatabaseService } from '../src/database/database.service';

let app: INestApplication;
let server: HttpServer;
let moduleFixture: TestingModule;
let cache: CacheService;
// let database: DatabaseService;

beforeAll(async () => {
  moduleFixture = await Test.createTestingModule({
    imports: [AppModule],
  }).compile();

  app = moduleFixture.createNestApplication();
  cache = moduleFixture.get<CacheService>(CacheService);
  // database = moduleFixture.get<DatabaseService>(DatabaseService);
  await app.init();
  server = app.getHttpServer();
});

afterEach(async () => {
  // await database.resetDb();
  await cache.reset();
});

afterAll(async () => {
  await app.close();
});

export { app, server };

```

There's another optimization we can make to this setup.

In the default test file that NestJS generates, you'll see that the `beforeEach` hook is used to create a new instance of the `Test` object, and then each test uses `app.getHttpServer()` to get the instance of the NestJS application and make requests to it.

```ts:app.e2e-spec.ts
import { Test, TestingModule } from '@nestjs/testing';
import { INestApplication } from '@nestjs/common';
import * as request from 'supertest';
import { AppModule } from './../src/app.module';

describe('AppController (e2e)', () => {
  let app: INestApplication;

  beforeEach(async () => {
    const moduleFixture: TestingModule = await Test.createTestingModule({
      imports: [AppModule],
    }).compile();

    app = moduleFixture.createNestApplication();
    await app.init();
  });

  it('/ (GET)', () => {
    return request(app.getHttpServer())
      .get('/')
      .expect(200)
      .expect({ data: 'Hello World!' });
  });
});
```

The thing is, in our setup file, we're already creating an app instance and initializing it. So we can just use this instance in our tests instead of creating a new instance of the `Test` object and getting the app instance from it. First export the server from the setup file:

```ts:setup.ts
import { HttpServer, INestApplication } from '@nestjs/common';
import { Test, TestingModule } from '@nestjs/testing';
import { AppModule } from '../src/app.module';
import { CacheService } from '../src/core/cache/cache.service';
import { DatabaseService } from '../src/database/database.service';

let app: INestApplication;
let server: HttpServer; // <--- Define the server instance variable
let moduleFixture: TestingModule;
let cache: CacheService;
// let database: DatabaseService;

beforeAll(async () => {
  moduleFixture = await Test.createTestingModule({
    imports: [AppModule],
  }).compile();

  app = moduleFixture.createNestApplication();
  cache = moduleFixture.get<CacheService>(CacheService);
  // database = moduleFixture.get<DatabaseService>(DatabaseService);
  await app.init();
  server = app.getHttpServer(); // <--- Get the server instance
});

afterEach(async () => {
  // await database.resetDb();
  await cache.reset();
});

afterAll(async () => {
  await app.close();
});

export { server }; // <--- Export the server instance

```

And now all the tests can use the server instance from the setup file:

```ts:app.e2e-spec.ts
import * as request from 'supertest';
import { server } from './setup';

describe('AppController (e2e)', () => {
  it('/ (GET)', () => {
    return request(server)
      .get('/')
      .expect(200)
      .expect({ data: 'Hello World!' });
  });
});

```

This is much cleaner and prevents a lot of duplicate module set up!

## Setting up a CI pipeline

Now that we have our tests set up, let's set up a CI pipeline in Github Actions to run them.

Specifically, our CI pipeline will automatically run the unit tests, integration & e2e tests on every pull request and push to the `main` branch.

In the root of your project, add a `.github` directory and the following sub directories and files:

```
.github
â””â”€â”€ actions
    â””â”€â”€ build
        â””â”€â”€ action.yml
â””â”€â”€ workflows
    â””â”€â”€ test.yml
```

In the `test.yml` file, add the following:

```yml:.github/workflows/test.yml
name: Tests
on: workflow_dispatch

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: ./.github/actions/build
      - name: Run tests
        run: pnpm test

```

The `on: workflow_dispatch` is set temporarily so we can test the worklfow in Github. We'll update it shortly to run on every pull request and push to the `main` branch.

You can see I've added a single job called `unit-tests` which has the following steps:

- The checkout action is used to fetch your repository's code, making it available for subsequent steps in the workflow.
- The build action points to a file (that we'll add below) that installs the dependencies in the repository
- And then finally, the command `pnpm test` is used to run the unit tests
