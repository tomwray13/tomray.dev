---
title: 'Real estate listings - NestJS Starter'
h1: Real estate listings - NestJS Starter
date: '2023-07-23'
lastmod: '2023-07-23'
draft: false
summary: Real estate listings - NestJS Starter
images: ['/static/images/nestjs-cheat-sheet/banner.png']
isHiddenFromSearch: false
---

Please make sure you've installed the pre-requisites before starting!

## Intro

We're going to build a real-estate API that allows you to create real estate listings with photos. The photos are added to a queue for processing in the background (where we'll upload the file to Google Cloud Storage and store the public image url in the database).

Here are the topics and features we'll cover in this project:

- Handling and validating file uploads
- Sending files to a queue as jobs
- Processing the jobs in the background
- Viewing the queue jobs in a UI
- Using Google Cloud Storage to store the files
- Validation with DTOs and pipes
- Unit testing, integration testing and end-to-end testing

Let's get started by cloning the starter repo:

```bash
git clone CLONE_URL
```

Let's dive in!

- handle multiple file uploads for listing images with basic validation
  -- show using multi-part/form data requires a tweak to the DTO to transform strings to respective numbers
  -- add a custom pipe to validate number of files - FilesInterceptor argument is possible but the returned error is not clear
- register queue and send off images for processing
  -- The forRoot() method is used to register a bull package configuration object that will be used by all queues registered in the application. Applied in CommonModule
  -- notes on registering queue against same instance as Redis cache
- implement Bull Arena for queue UI
- add consumer to send each image to the queue
- Use GCS to upload the file
  -- create a bucket (ensure you choose fine-grained access control so you can set public:true)
  -- create service account with Storage Object Admin
  -- create GCS service and method to upload file, using @google-cloud/storage package
- Add schema for listing image and handle in consumer

## Add a listings module

Let's start by adding a listings resource. We'll use the Nest CLI to generate the module, service and controller:

```bash
npx nest generate resource modules/listing
```

Use REST API in the options and choose Y for generating CRUD entry points.

We're going to focus on the creating listings, so you can remove the other methods. You should be left with this in a listings controller:

```ts
import { Controller, Post, Body } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  create(@Body() createListingDto: CreateListingDto) {
    return this.listingService.create(createListingDto)
  }
}
```

And this in a listings service:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'

@Injectable()
export class ListingService {
  create(createListingDto: CreateListingDto) {
    return 'This action adds a new listing'
  }
}
```

In the `listing` folder you can delete the following files:

`listing.service.spec.ts` - this is the unit testing file, we'll cover this later
`listing.controller.spec.ts` - this is the testing file for the controller, we'll cover this later in e2e tests
`entities/` - entities are for TypeORM which we're not using (we use Prisma)

You can also remove the default `app.controller.ts`, `app.controller.spec.ts` and `app.service.ts` files as these won't be needed. You'll need to remove the files and there reference in the `app.module.ts`.

Your `AppModule` should look like this:

```ts
import { Module } from '@nestjs/common'
import { CoreModule } from './core/core.module'
import { DatabaseModule } from './database/database.module'
import { ListingModule } from './modules/listing/listing.module'

@Module({
  imports: [CoreModule, DatabaseModule, ListingModule],
  controllers: [],
  providers: [],
})
export class AppModule {}
```

## Storing listings in the database

We need a way of storing the listings in the database. We'll use Prisma for this which was set up in the starter repo project.

Head into the `schema.prisma` file in the `database` directory and add the following schema:

```prisma
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Listing {
  id              Int       @id @default(autoincrement())
  label           String
  addressLine1    String
  addressLine2    String?
  addressCity     String
  addressZipcode  String
  addressState    String
  price           Int
  bathrooms       Int
  bedrooms        Int
  squareMeters    Int
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
}
```

There will be a `User` model which is included in the starter repo by default - you can delete this model.

The schema is pretty self-explanatory. We have a `Listing` model with a number of fields.

- We're using the `@default` directive to set the `createdAt` field to the current date and time when a record is created.
- We're using the `@updatedAt` directive to set the `updatedAt` field to the current date and time when a record is updated.
- All other fields are required except for `addressLine2` which is optional.

To apply these database changes, we need to run our local migration command:

```bash
pnpm db:migrate:dev
```

You'll be prompted to name the migration. You can call it "initial migration".

You might be wondering about the listing images. We're going to store the images in Google Cloud Storage and store the public url in the database in a seperate table. We'll cover this later.

So when our `POST /listing` endpoint is used, we want to create a new listing in the database. This means the data received in the request body needs to match the schema above.

When we created the listing resource earlier, it created a DTO for us - `create-listing.dto.ts`. DTOs are for validation and transformation - perfect for this use case.

Let's update this now to manually match the schema:

```ts
import { IsInt, IsNotEmpty, IsOptional, IsString, Min } from 'class-validator'

export class CreateListingDto {
  @IsString()
  @IsNotEmpty()
  label: string

  @IsString()
  @IsNotEmpty()
  addressLine1: string

  @IsString()
  @IsNotEmpty()
  @IsOptional()
  addressLine2: string

  @IsString()
  @IsNotEmpty()
  addressCity: string

  @IsString()
  @IsNotEmpty()
  addressZipcode: string

  @IsInt()
  @Min(0)
  price: number

  @IsInt()
  @Min(0)
  bathrooms: number

  @IsInt()
  @Min(0)
  bedrooms: number

  @IsInt()
  @Min(0)
  squareMeters: number
}
```

Spin up your local server and give the `POST /listing` endpoint a test to make sure it's working as expected. You can use this JSON as an example:

```json
{
  "label": "Spacious Apartment in Downtown",
  "addressLine1": "123 Main St",
  "addressLine2": "Apt 4B",
  "addressCity": "Springfield",
  "addressZipcode": "62704",
  "price": 150000,
  "bathrooms": 2,
  "bedrooms": 3,
  "squareMeters": 120
}
```

Pass in different values and make sure you get the expected validation errors.

Before we implement the logic to actually store the listing in the database, we can make an optimisation to the DTO we just created.

The purpose of the DTO is to align the data received in the request body with the schema in the database. Right now, we've manually defined the DTO to match the schema. This is fine, but it's a bit repetitive and can be error-prone.

Prisma generates some types from the schema that we can leverage to make this easier.

If you review the database schema again, we actually need the user to provide all fields except for `id`, `createdAt` and `updatedAt`:

```prisma
// ...

model Listing {
  id              Int       @id @default(autoincrement()) // generated automatically
  label           String
  addressLine1    String
  addressLine2    String?
  addressCity     String
  addressZipcode  String
  addressState    String
  price           Int
  bathrooms       Int
  bedrooms        Int
  squareMeters    Int
  createdAt       DateTime  @default(now()) // generated automatically
  updatedAt       DateTime  @updatedAt // generated automatically
}
```

Inside the `modules/listing` directory, add a new file called `listing.types.ts` and add the following:

```ts
import { Listing as PrismaListing } from '@prisma/client'

export type CreateListingInput = Omit<PrismaListing, 'id' | 'createdAt' | 'updatedAt'>
```

This creates a type called `CreateListingInput` which is the same as the `Listing` type generated by Prisma, except it omits the `id`, `createdAt` and `updatedAt` fields.

This is super helpful because now we can use this type in our DTO and we don't have to manually define the DTO to match the schema. We can do this by using `implements` - this allows us to implement an interface or type in a class or DTO.

```ts
import { IsInt, IsNotEmpty, IsOptional, IsString, Min } from 'class-validator'
import { CreateListingInput } from '../listing.types'

export class CreateListingDto implements CreateListingInput {
  @IsString()
  @IsNotEmpty()
  label: string

  @IsString()
  @IsNotEmpty()
  addressLine1: string

  @IsString()
  @IsNotEmpty()
  @IsOptional()
  addressLine2: string

  @IsString()
  @IsNotEmpty()
  addressCity: string

  @IsString()
  @IsNotEmpty()
  addressZipcode: string

  @IsInt()
  @Min(0)
  price: number

  @IsInt()
  @Min(0)
  bathrooms: number

  @IsInt()
  @Min(0)
  bedrooms: number

  @IsInt()
  @Min(0)
  squareMeters: number
}
```

You should see a type error in your IDE:

```bash
Class 'CreateListingDto' incorrectly implements interface 'CreateListingInput'.
  Property 'addressState' is missing in type 'CreateListingDto' but required in type 'CreateListingInput'.
```

This is exactly why implementing the type is better than manually defining the DTO. We get type safety and the code is more readable! We forgot to include the `addressState` in the DTO so let's add that now:

```ts
import { IsInt, IsNotEmpty, IsOptional, IsString, Min } from 'class-validator'
import { CreateListingInput } from '../listing.types'

export class CreateListingDto implements CreateListingInput {
  @IsString()
  @IsNotEmpty()
  label: string

  @IsString()
  @IsNotEmpty()
  addressLine1: string

  @IsString()
  @IsNotEmpty()
  @IsOptional()
  addressLine2: string

  @IsString()
  @IsNotEmpty()
  addressCity: string

  @IsString()
  @IsNotEmpty()
  addressZipcode: string

  @IsString()
  @IsNotEmpty()
  addressState: string

  @IsInt()
  @Min(0)
  price: number

  @IsInt()
  @Min(0)
  bathrooms: number

  @IsInt()
  @Min(0)
  bedrooms: number

  @IsInt()
  @Min(0)
  squareMeters: number
}
```

Let's now update the `listing.service.ts` to use the DTO and create a new listing in the database:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'

@Injectable()
export class ListingService {
  constructor(private readonly databaseService: DatabaseService) {}

  async create(createListingDto: CreateListingDto) {
    return await this.databaseService.listing.create({
      data: createListingDto,
    })
  }
}
```

We've injected the `DatabaseService` to create the listing, and passed in the `CreateListingDto` to the `create` method. As it matches the `CreateListingInput` type, we don't need to do any manual mapping!

In order to inject any dependency outside of the current module, we need to ensure we've imported the respective module in the `imports` array of the `@Module` decorator in the `listing.module.ts` file:

```ts
import { Module } from '@nestjs/common'
import { ListingService } from './listing.service'
import { ListingController } from './listing.controller'
import { DatabaseModule } from '../../database/database.module'

@Module({
  imports: [DatabaseModule], // <-- add this
  controllers: [ListingController],
  providers: [ListingService],
})
export class ListingModule {}
```

Nice! Give the method a test and make sure it's working as expected.

You can spin up `pnpm db:studio` to view the database in the Prisma Studio UI and confirm the records been added.

## Attach files to POST /listing request

So, we've set up an endpoint and logic to create a real-estate listing.

Let's update the endpoint to allow the user to upload images for the listing. In the next step we'll add the logic to process the images in the background, but for now we'll just attach the files to the request.

To attach file(s) to an HTTP request, the standard way to do this is by using multipart/form-data as the content type. This content type allows for the transmission of both text and binary data in a single request.

NestJS recommends installing the @types/multer package so you have better type safety when handle multipart/form-data requests:

```bash
pnpm add -D @types/multer
```

To enable sending multipart/form-data, we need to use a built-in NestJS interceptor called `FilesInterceptor`. This interceptor allows us to intercept the request and attach files to the request body:

```ts
import { Controller, Post, Body, UseInterceptors, UploadedFiles } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'
import { FilesInterceptor } from '@nestjs/platform-express'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  @UseInterceptors(FilesInterceptor(`images`))
  create(
    @Body() createListingDto: CreateListingDto,
    @UploadedFiles() files: Express.Multer.File[]
  ) {
    console.log(`Uploaded files:`, files)
    return this.listingService.create(createListingDto)
  }
}
```

Also note that we're using the `@UploadedFiles()` decorator. This is a built-in decorator that allows us to access the files that were attached to the request.

With this setup, try sending a request to the endpoint with a file attached. Instead of sending the request body in JSON or form-urlencoded, you'll need to send it as `multipart/form-data`. If you want to attach files to an HTTP request, you'll need to use `multipart/form-data` as the content type (it's also how the `FilesInterceptor` knows to intercept the request).

You should get a 400 error back!

```json
{
  "message": [
    "price must not be less than 0",
    "price must be an integer number",
    "bathrooms must not be less than 0",
    "bathrooms must be an integer number",
    "bedrooms must not be less than 0",
    "bedrooms must be an integer number",
    "squareMeters must not be less than 0",
    "squareMeters must be an integer number"
  ],
  "error": "Bad Request",
  "statusCode": 400
}
```

This is because the `CreateListingDto` expects the `price`, `bathrooms`, `bedrooms` and `squareMeters` fields to be numbers, but they're being sent as strings.

When using `multipart/form-data` as the content type, all the respective fields are sent as strings, which is why we're getting the validation error.

Let's update the DTO to transform the strings to numbers:

```ts
import { IsInt, IsNotEmpty, IsOptional, IsString, Min } from 'class-validator'
import { Type } from 'class-transformer'
import { CreateListingInput } from '../listing.types'

export class CreateListingDto implements CreateListingInput {
  // ...

  @IsInt()
  @Min(0)
  @Type(() => Number)
  price: number

  @IsInt()
  @Min(0)
  @Type(() => Number)
  bathrooms: number

  @IsInt()
  @Min(0)
  @Type(() => Number)
  bedrooms: number

  @IsInt()
  @Min(0)
  @Type(() => Number)
  squareMeters: number
}
```

The `Type` function provided by the `class-transformer` package allows us to transform the string to a number.

Give it a test and the files you attached should now be logged to the console!

Instead of logging the file in the controller, let's include it in the payload for the service method to keep all the logic in the service layer:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'

@Injectable()
export class ListingService {
  constructor(private readonly databaseService: DatabaseService) {}

  async create({ data, images }: { data: CreateListingDto; images: Express.Multer.File[] }) {
    const listing = await this.databaseService.listing.create({
      data,
    })
    for (const image of images) {
      // do something with image
      console.log(`image`, image)
    }
    return listing
  }
}
```

And then update the controller to pass the files to the service:

```ts
import { Controller, Post, Body, UseInterceptors, UploadedFiles } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'
import { FilesInterceptor } from '@nestjs/platform-express'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  @UseInterceptors(FilesInterceptor(`images`))
  create(
    @Body() createListingDto: CreateListingDto,
    @UploadedFiles() files: Express.Multer.File[]
  ) {
    return this.listingService.create({
      data: createListingDto,
      images: files,
    })
  }
}
```

Nice! Let's now consider some validation we'd like to do on the file uploads:

- We only want to allow images to be uploaded (i.e. png, jpeg and jpg files)
- The files should have a max size of 1MB
- We only want to allow a maximum of 10 images to be uploaded

To achieve this validation, we can use NestJS Pipes. In fact, NestJS has some built-in pipes to help with some of the above validation. Specifically, there's the ParseFilePipe that allows us to pass in 'validators'.

NestJS has 2 built in validators, `FileTypeValidator` and `MaxFileSizeValidator`. Let's use these to validate the file type and max file size:

```ts
import { Controller, Post, Body, UseInterceptors, UploadedFiles } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'
import { FilesInterceptor } from '@nestjs/platform-express'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  @UseInterceptors(FilesInterceptor(`images`))
  create(
    @Body() createListingDto: CreateListingDto,
    @UploadedFiles(
      new ParseFilePipe({
        validators: [
          new FileTypeValidator({ fileType: `.(png|jpeg|jpg)` }),
          new MaxFileSizeValidator({ maxSize: 1024 * 1024 }), // 1MB
        ],
      })
    )
    files: Express.Multer.File[]
  ) {
    return this.listingService.create({
      data: createListingDto,
      images: files,
    })
  }
}
```

Make a few requests and you should see the validation errors as expected. For example, file sizes over 1MB and file types that are not png, jpeg or jpg should return a 400 error.

There's 1 more validation layer we want to add: we only want to allow a maximum of 10 images to be uploaded.

In the NestJS documentation, it mentions the `FilesInterceptor` accepts a number to define the maximum number of images which you can apply like this:

```ts
import { Controller, Post, Body, UseInterceptors, UploadedFiles } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'
import { FilesInterceptor } from '@nestjs/platform-express'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  @UseInterceptors(FilesInterceptor(`images`, 10)) // <-- add argument for max file count
  // ....
}
```

However, if you make a request exceeding this file count, you'll notice that the validation errors are not very helpful:

```json
{
  "message": "Unexpected field",
  "error": "Bad Request",
  "statusCode": 400
}
```

This isn't very helpful for the client making the request, so let's instead add our own custom pipe to handle this validation.

Other modules might need this pipe in the future, so let's add a directory called `common` inside `src`. This `common` directory will contain any shared code that can be used across the application.

Inside the `common` directory, add a new directory called `pipes` and inside that add a new file called `max-file-count.pipe.ts`:

```ts
import { PipeTransform, Injectable, BadRequestException } from '@nestjs/common'

@Injectable()
export class MaxFileCountValidationPipe implements PipeTransform {
  constructor(private readonly maxFiles: number) {}
  transform(files: Express.Multer.File[]) {
    if (files.length > this.maxFiles) {
      throw new BadRequestException(`Maximum of ${this.maxFiles} file uploads allowed`)
    }
    return files
  }
}
```

Let's now remove the argument we added earlier in the `FilesInterceptor` and instead apply this pipe to the `@UploadedFiles()` decorator:

```ts
import { Controller, Post, Body, UseInterceptors, UploadedFiles } from '@nestjs/common'
import { ListingService } from './listing.service'
import { CreateListingDto } from './dto/create-listing.dto'
import { FilesInterceptor } from '@nestjs/platform-express'

@Controller('listing')
export class ListingController {
  constructor(private readonly listingService: ListingService) {}

  @Post()
  @UseInterceptors(FilesInterceptor(`images`)) // <-- remove max count argument
  create(
    @Body() createListingDto: CreateListingDto,
    @UploadedFiles(
      new ParseFilePipe({
        validators: [
          new FileTypeValidator({ fileType: `.(png|jpeg|jpg)` }),
          new MaxFileSizeValidator({ maxSize: 1024 * 1024 }),
        ],
      }),
      new MaxFileCountValidationPipe(10) // <-- add this
    )
    files: Express.Multer.File[]
  ) {
    return this.listingService.create({
      data: createListingDto,
      images: files,
    })
  }
}
```

With some validation setup on the incoming attached files, let's now dive into the queue set up!

## Set up the Bull Queue

To set up a queue in NestJS we're going to use [Bull](https://github.com/OptimalBits/bull). Bull is a Node.js package that allows you to create and manage queues and is powered by Redis.

Bull is a Node.js package that allows you to create and manage queues and is powered by Redis.

In the NestJS starter repo, we already set up a cache module (which uses Redis), so there is a question around should we use the same Redis instance for the queue or a seperate one.

In many cases, starting with a single Redis instance might be okay during development or in low-traffic scenarios, but as the application scales, moving to separate instances might become necessary. So we're going to stick with a single Redis instance for now.

Before diving in, let's install the relevant Bull packages:

```bash
pnpm add bull @nestjs/bull
```

To set up Bull in the project, we need to register a globally available configuration for all queues. This connects to the Redis instance and sets up some default options for all queues.

As this setup is global, we're going to add it to the `CoreModule`.

Let's add a new `Queue` module that's imported to the `CoreModule` by running this command in the terminal:

```bash
npx nest generate module core/queue
```

Inside of the `queue.module.ts` file, let's register the global Bull configuration. We'll need to inject the `ConfigService` so we can access the Redis instance credentials:

```ts
import { Module } from '@nestjs/common'
import { BullModule } from '@nestjs/bull'
import { ConfigModule, ConfigService } from '@nestjs/config'

@Module({
  imports: [
    BullModule.forRootAsync({
      imports: [ConfigModule],
      useFactory: async (configService: ConfigService) => {
        const username = configService.get('redis.username')
        const password = configService.get('redis.password')
        return {
          redis: {
            host: configService.get('redis.host'),
            port: configService.get('redis.port'),
            ...(username && { username }),
            ...(password && { password }),
          },
        }
      },
      inject: [ConfigService],
    }),
  ],
})
export class QueueModule {}
```

We want to create a queue where we can send jobs to do the following:

- Upload the file to Google Cloud Storage, getting the public image url
- Store the respective public url in the database.

Let's start by registering a queue dedicated for the listing module.

Inside `listing.module.ts`, register a new queue with name `listing`:

```ts
import { Module } from '@nestjs/common'
import { ListingService } from './listing.service'
import { ListingController } from './listing.controller'
import { DatabaseModule } from '../../database/database.module'
import { BullModule } from '@nestjs/bull'

@Module({
  imports: [BullModule.registerQueue({ name: `listing` }), DatabaseModule],
  controllers: [ListingController],
  providers: [ListingService],
})
export class ListingModule {}
```

We need a way of adding jobs to the listing queue and processing jobs from the listing queue.

Inside the `listing` directory, add a new directory called `queue` and inside that add 2 new files:

1. `listing.producer.ts` - this will be for adding jobs to the queue
1. `listing.consumer.ts` - this will be for processing jobs from the queue

Let's start with the producer. Inside `listing.producer.ts`, add the following:

```ts
import { Injectable } from '@nestjs/common'
import { InjectQueue } from '@nestjs/bull'
import { Queue } from 'bull'

@Injectable()
export class ListingProducer {
  constructor(@InjectQueue(`listing`) private listingQueue: Queue) {}

  async createListingImage(image: Express.Multer.File) {
    return await this.listingQueue.add(`createListingImage`, image)
  }
}
```

Here's how the @InjectQueue decorator works:

- The decorator accepts a string argument which is the name of the queue we want to inject
- The decorator injects the queue instance into the class
- We can then use the queue instance to add jobs to the queue

For now, the payload just passes through a string but we'll tweak this shortly.

And let's add the consumer:

```ts
import { Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'

@Processor(`listing`)
export class ListingConsumer {
  @Process(`createListingImage`)
  async createListingImage(job: Job<Express.Multer.File>) {
    console.log(`createListingImage filename:`, job.data.originalname)
    return job.data
  }
}
```

Let's not forget to register the producer and consumer in the `listing.module.ts` file:

```ts
import { Module } from '@nestjs/common'
import { ListingService } from './listing.service'
import { ListingController } from './listing.controller'
import { DatabaseModule } from '../../database/database.module'
import { BullModule } from '@nestjs/bull'
import { ListingProducer } from './queue/listing.producer'
import { ListingConsumer } from './queue/listing.consumer'

@Module({
  imports: [BullModule.registerQueue({ name: `listing` }), DatabaseModule],
  controllers: [ListingController],
  providers: [ListingService, ListingProducer, ListingConsumer], // <-- add producer and consumer
})
export class ListingModule {}
```

Let's now update the `listing.service.ts` to use the producer to add a job to the queue:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'
import { ListingProducer } from './queue/listing.producer'

@Injectable()
export class ListingService {
  constructor(
    private readonly databaseService: DatabaseService,
    private readonly listingQueue: ListingProducer
  ) {}

  async create({ data, images }: { data: CreateListingDto; images: Express.Multer.File[] }) {
    const listing = await this.databaseService.listing.create({
      data,
    })
    for (const image of images) {
      // send image to queue
      await this.listingQueue.createListingImage(image)
    }
    return listing
  }
}
```

Run another HTTP request and you should see the job data logged to the console.

Nice work, you've just set up a queue and added a job to it!

Before getting onto adding the logic for handling the file uploads, there are a couple of things we can do to improve our current queue set up:

1. The queue name `listing` is hard-coded in multiple places. We should use a constant instead so we can easily change the name in one place.
2. What happens if the job consumer throws an error? Let's setup some logging to automatically log all errors that happen in the queue.

Let's start with the queue name. Inside the `core/queue` directory, add a new file called `queue.constants.ts` with the following:

```ts
export const LISTING_QUEUE = `listing`
```

Now use this constant in the `listing.module.ts`:

```ts
import { Module } from '@nestjs/common'
import { ListingService } from './listing.service'
import { ListingController } from './listing.controller'
import { DatabaseModule } from '../../database/database.module'
import { BullModule } from '@nestjs/bull'
import { ListingProducer } from './queue/listing.producer'
import { ListingConsumer } from './queue/listing.consumer'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'

@Module({
  imports: [BullModule.registerQueue({ name: LISTING_QUEUE }), DatabaseModule], // <-- use constant
  controllers: [ListingController],
  providers: [ListingService, ListingProducer, ListingConsumer],
})
export class ListingModule {}
```

In the producer:

```ts
import { Injectable } from '@nestjs/common'
import { InjectQueue } from '@nestjs/bull'
import { Queue } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'

@Injectable()
export class ListingProducer {
  constructor(@InjectQueue(LISTING_QUEUE) private listingQueue: Queue) {}

  async createListingImage(image: Express.Multer.File) {
    return await this.listingQueue.add(`createListingImage`, image)
  }
}
```

And the consumer:

```ts
import { Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'

@Processor(LISTING_QUEUE)
export class ListingConsumer {
  @Process(`createListingImage`)
  async createListingImage(job: Job<Express.Multer.File>) {
    console.log(`createListingImage filename:`, job.data.originalname)
    return job.data
  }
}
```

In regards to setting up automatic logs for errors that happen when jobs are processed, we can leverage Bull events.

The NestJS bull package has quite a few events included out the box, like `@OnQueueFailed()` which is fired when a job fails to be processed and `@OnQueueCompleted` which is fired when a job is successfully processed.

Let's inject our custom logger and use the `@OnQueueFailed()` event to log any errors that happen when processing a job.

```ts
import { Process, Processor, OnQueueFailed } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'
import { Logger } from '../../core/logger/logger.service'

@Processor(LISTING_QUEUE)
export class ListingConsumer {
  constructor(private readonly logger: Logger) {}

  @Process(`createListingImage`)
  async createListingImage(job: Job<Express.Multer.File>) {
    console.log(`createListingImage filename:`, job.data.originalname)
    return job.data
  }

  @OnQueueFailed()
  async onError(job: Job<string>, error: any) {
    this.logger.error(
      `Failed job ${job.id} of type ${job.name}: ${error.message}`,
      error.stack,
      `Queue`,
      job.data
    )
  }
}
```

If you'd like to give it a test, throw an error in the `createListingImage` method and you should see the error logged to the console.

This works well, but as soon we add more jobs and/or queues, we'll have to repeat this code for each queue.

A nicer way of handling this is by creating a BaseConsumer class that all other consumers can extend from. This way, we can define the error handling logic in one place and all other consumers will inherit it.

In the `core/queue` directory, add a new file called `base.consumer.ts`:

```ts
import { OnQueueFailed } from '@nestjs/bull'
import { Job } from 'bull'
import { Logger } from '../logger/logger.service'

export abstract class BaseConsumer {
  constructor(protected readonly logger: Logger) {}
  @OnQueueFailed()
  onError(job: Job<any>, error: any) {
    this.logger.error(
      `Failed job ${job.id} of type ${job.name}: ${error.message}`,
      error.stack,
      `Queue`,
      job.data
    )
  }
}
```

Note a couple of things:

- I've used `abstract` when defining the class. This is because we don't ever want to instantiate this class directly, we just want to extend from it.
- For the logger, I've used `protected` instead of `private`. This is because we want to be able to access the logger in the classes that extend from this class.

Let's now update the `listing.consumer.ts` to extend from this class:

```ts
import { Process, Processor, OnQueueFailed } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'
import { Logger } from '../../core/logger/logger.service'
import { BaseConsumer } from '../../core/queue/base.consumer'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(logger: Logger) {
    super(logger)
  }

  @Process(`createListingImage`)
  async createListingImage(job: Job<Express.Multer.File>) {
    console.log(`createListingImage filename:`, job.data.originalname)
    return job.data
  }
}
```

Note I've had to remove the `private readonly` from the constructor argument as this is not allowed when using `super` in a constructor.

Nice! Our error logging inside jobs is not centralised.

On a side note, there might be a scenario where you want to add some logic to the `onError` method in a specific consumer. In this case, you can override the `onError` method in the consumer and add the logic there like this:

```ts
import { Process, Processor, OnQueueFailed } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'
import { Logger } from '../../core/logger/logger.service'
import { BaseConsumer } from '../../core/queue/base.consumer'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(logger: Logger) {
    super(logger)
  }

  @Process(`createListingImage`)
  async createListingImage(job: Job<Express.Multer.File>) {
    console.log(`createListingImage filename:`, job.data.originalname)
    return job.data
  }

  @OnQueueFailed()
  async onError(job: Job<string>, error: any) {
    super.onError(job, error)
    // do something else here
  }
}
```

Great work! We've setup a queue and added a job to it.

## Adding the logic to process the file uploads

Now that we've set up the queue, let's add the logic to process the file uploads. On a high level, here's what we'd like to within the job:

- Upload the file to Google Cloud Storage, getting the public image url
- Store the respective public url in the database associated with the listing

Right now, we're sending the file name to the queue:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'
import { ListingProducer } from './queue/listing.producer'

@Injectable()
export class ListingService {
  constructor(
    private readonly databaseService: DatabaseService,
    private readonly listingQueue: ListingProducer
  ) {}

  async create({ data, images }: { data: CreateListingDto; images: Express.Multer.File[] }) {
    const listing = await this.databaseService.listing.create({
      data,
    })
    for (const image of images) {
      // send image to queue
      await this.listingQueue.createListingImage(image.originalname)
    }
    return listing
  }
}
```

This was just a placeholder while we set up the queue.

To execute the logic we just mentioned, we'll need to pass the following:

- The file itself
- The mime type of the file
- The associated listing id

We have a couple of options for how we can pass the file to the queue:

- Send the file Buffer to the queue (the file in binary format)
- Convert the file Buffer to a string and send that to the queue

Redis is optimized for handling small bits of data, such as strings. So, converting the file Buffer into a string and sending that string to the Redis queue for processing is a valid approach to reduce the job size and can be beneficial for Redis.

In the queue itself, we'll then convert the string back to a Buffer and upload it to Google Cloud Storage.

Let's create a new DTO for this payload called `UploadListingImageDto`:

```ts
import { Listing } from '@prisma/client'
import { IsInt, IsNotEmpty, IsString } from 'class-validator'

export class UploadListingImageDto {
  @IsString()
  @IsNotEmpty()
  base64File: string

  @IsString()
  @IsNotEmpty()
  mimeType: string

  @IsInt()
  listingId: Listing[`id`]
}
```

Leverage a DTO for the payload is a good idea as it allows us to add type checking and also to use a consistent payload across the application.

Let's update the `ListingProducer` to use this DTO:

```ts
import { Injectable } from '@nestjs/common'
import { InjectQueue } from '@nestjs/bull'
import { Queue } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'

@Injectable()
export class ListingProducer {
  constructor(@InjectQueue(LISTING_QUEUE) private listingQueue: Queue) {}

  async uploadListingImage(payload: UploadListingImageDto) {
    return await this.listingQueue.add(`createListingImage`, payload)
  }
}
```

We should also update the `ListingConsumer` to use this DTO:

```ts
import { Process, Processor, OnQueueFailed } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'
import { Logger } from '../../core/logger/logger.service'
import { BaseConsumer } from '../../core/queue/base.consumer'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(logger: Logger) {
    super(logger)
  }

  @Process(`createListingImage`)
  async createListingImage(job: Job<UploadListingImageDto>) {
    console.log(`createListingImage job data:`, job.data)
    return job.data
  }

  // ...
}
```

And finally, we'll need to update the `ListingService` to use this DTO:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'
import { ListingProducer } from './queue/listing.producer'

@Injectable()
export class ListingService {
  constructor(
    private readonly databaseService: DatabaseService,
    private readonly listingQueue: ListingProducer
  ) {}

  async create({ data, images }: { data: CreateListingDto; images: Express.Multer.File[] }) {
    const listing = await this.databaseService.listing.create({
      data,
    })
    for (const image of images) {
      await this.listingQueue.uploadListingImage({
        base64File: `TODO`, // <-- we'll get to this next
        mimeType: image.mimetype,
        listingId: listing.id,
      })
    }
    return listing
  }
}
```

As noted above, we're going to convert the Buffer to a string and send that to the queue, then convert the string back to a Buffer in the queue.

Let's add a utilities module and respective `FileService` for these file conversion methods:

```bash
npx nest generate module utilities
npx nest generate service utilities/file
```

And in the `FileService`:

```ts
import { Injectable } from '@nestjs/common'

@Injectable()
export class FileService {
  bufferToBase64(buffer: Buffer): string {
    return buffer.toString(`base64`)
  }

  base64ToBuffer(base64String: string): Buffer {
    return Buffer.from(base64String, `base64`)
  }
}
```

Now let's update the `listing.service.ts` to convert the file Buffer to a string and send that to the queue:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'
import { ListingProducer } from './queue/listing.producer'
import { FileService } from '../../utilities/file/file.service'

@Injectable()
export class ListingService {
  constructor(
    private readonly databaseService: DatabaseService,
    private readonly listingQueue: ListingProducer,
    private readonly fileService: FileService
  ) {}

  async create({ data, images }: { data: CreateListingDto; images: Express.Multer.File[] }) {
    const listing = await this.databaseService.listing.create({
      data,
    })
    for (const image of images) {
      await this.listingQueue.uploadListingImage({
        base64File: this.fileService.bufferToBase64(image.buffer),
        mimeType: image.mimetype,
        listingId: listing.id,
      })
    }
    return listing
  }
}
```

As we're using the `FileService`, we'll need to import the respective module into the `ListingModule`:

```ts
import { Module } from '@nestjs/common'
import { ListingService } from './listing.service'
import { ListingController } from './listing.controller'
import { DatabaseModule } from '../../database/database.module'
import { BullModule } from '@nestjs/bull'
import { ListingProducer } from './queue/listing.producer'
import { ListingConsumer } from './queue/listing.consumer'
import { LISTING_QUEUE } from '../../core/queue/queue.constants'
import { UtilitiesModule } from '../../utilities/utilities.module'

@Module({
  imports: [
    BullModule.registerQueue({ name: LISTING_QUEUE }),
    DatabaseModule,
    UtilitiesModule, // <-- add this
  ],
  controllers: [ListingController],
  providers: [ListingService, ListingProducer, ListingConsumer],
})
export class ListingModule {}
```

Let's now update the consumer to first convert the string back to a Buffer:

```ts
import { OnQueueFailed, Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'
import { Logger } from '../../../core/logger/logger.service'
import { BaseConsumer } from '../../../core/queue/base.consumer'
import { FileService } from '../../../utilities/file/file.service'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(logger: Logger, private readonly fileService: FileService) {
    super(logger)
  }

  @Process(`createListingImage`)
  async uploadListingImage(job: Job<UploadListingImageDto>) {
    const { base64File } = job.data
    const buffer = this.fileService.base64ToBuffer(base64File)
    // TODO: upload file to Google Cloud Storage
    // TODO: store public url in database
    return job.data
  }

  // ...
}
```

Let's now add the logic to upload the file to Google Cloud Storage.

Let's next install the Google Cloud Storage package:

```bash
pnpm add @google-cloud/storage
```

Google Cloud Storage allows us to upload and retrieve files from _buckets_.

You'll need to have an existing Google Cloud project set up before proceeding. Here's [the link](https://console.cloud.google.com/projectcreate) to creating a new project if you need it.

Inside the [Google Cloud console](https://console.cloud.google.com), open up the navigation and head to _Cloud Storage_ and then _Buckets_:

IMAGE

You might be prompted to "Enable Billing", which is necessary if you'd like to use Google Cloud Buckets.

Next, press the **Create Bucket** button:

IMAGE

Go through the form and keep the default settings, except for when you reach the section on "Choose how to control access to objects" - you should uncheck the box "Enforce public access prevention on this bucket".

Once you're bucket is created, head to the _Permissions_ tab and add a new permission:

![Google Cloud Storage bucket permissions](/static/images/google-cloud-file-uploads/google-cloud-storage-permissions.png)

Search and select _allUsers_, then choose the role _Storage Object Viewer_:

![Select allUsers and Storage Object Viewers permissions](/static/images/google-cloud-file-uploads/google-cloud-storage-permissions-allUsers.png)

Setting these configurations means that all files uploaded are accessible via a public link that we'll store in the database.

In order to start uploading files to the bucket, we need to set up some credentials (a service account key) with Google Cloud. Only the people who have access to these credentials will be able to access your bucket.

Head back into the navigation, go to _APIs and services_ then select _Credentials_:

![Navigate to APIs and services](/static/images/google-cloud-file-uploads/google-cloud-navigation-credentials.png)

You'll then want to press the _Create Credentials_ button and select _Service account_:

![Create Google Cloud credentials](/static/images/google-cloud-file-uploads/google-cloud-create-credentials.png)

Give your service account a name and hit _Create and continue_:

![Create a service account](/static/images/google-cloud-file-uploads/google-cloud-service-account.png)

In the next step, you'll need to apply a role to this service account to define the kind of access it has. You'll want to scroll to _Cloud Storage_ and then choose _Storage Admin_, which gives you full read and write operations to Google Cloud Storage:

![Enable Storage Admin permissions to service account](/static/images/google-cloud-file-uploads/google-cloud-service-account-access.png)

You can skip the final section.

Once your service account has been created, go into the service account and navigate to the _Keys_ tab. You want to create a key for this service account which gives you the private, unique information you need to make read/write operations:

![Navigate to the service account keys tab](/static/images/google-cloud-file-uploads/google-cloud-service-account-keys.png)

Then create a new key. When prompted, choose the JSON option, which will download the file to your local machine:

![Download service account JSON key](/static/images/google-cloud-file-uploads/google-cloud-service-account-create-key.png)

Most tutorials suggest adding this JSON file to the root of your project and adding it to your `gitignore` file (because this file contains sensitive keys to access your Google Cloud account).

However, I prefer to instead grab the relevant keys from the JSON file and leverage env variables. The main reason because it's much simpler for deployment - you just need to add the env variables in each environment.

Let's now go back into the NestJS project and define some new env variables in the `config` file:

```ts
export default () => ({
  // ...
  googleCloud: {
    projectId: process.env.GCS_PROJECT_ID,
    clientEmail: process.env.GCS_CLIENT_EMAIL,
    privateKey: process.env.GCS_PRIVATE_KEY
      ? process.env.GCS_PRIVATE_KEY.replace(/\\n/g, '\n')
      : ``,
  },
})
```

In your env file, add the respective values for `GCS_PROJECT_ID`, `GCS_CLIENT_EMAIL` and `GCS_PRIVATE_KEY`.

- `GCS_PROJECT_ID` is the project ID of your Google Cloud project
- `GCS_CLIENT_EMAIL` is the client email of the service account you created
- `GCS_PRIVATE_KEY` is the private key of the service account you created. You can find this in JSON file you downloaded under the `private_key` key

Let's now create a `GoogleCloudService` module and service to handle the file uploads.

In a new directory called `/services` add a sub directory called `google-cloud` and inside that add 2 new files:

1. `google-cloud.module.ts`
2. `google-cloud.service.ts`

In the `google-cloud.module.ts` file, add the following:

```ts
import { Module } from '@nestjs/common'
import { GoogleCloudService } from './google-cloud.service'

@Module({
  providers: [GoogleCloudService],
  exports: [GoogleCloudService],
})
export class GoogleCloudModule {}
```

And then in the `google-cloud.service.ts` file:

```ts
import { Storage } from '@google-cloud/storage'
import { Injectable } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'

@Injectable()
export class GoogleCloudService {
  private storage: Storage
  private bucket: string
  constructor(private readonly configService: ConfigService) {
    this.storage = new Storage({
      projectId: this.configService.getOrThrow(`googleCloud.projectId`),
      credentials: {
        client_email: this.configService.getOrThrow(`googleCloud.clientEmail`),
        private_key: this.configService.getOrThrow(`googleCloud.privateKey`),
      },
    })

    this.bucket = `bucketname` // <-- add your bucket name here
  }
}
```

What we've done here is:

- Imported the `Storage` class from the `@google-cloud/storage` package
- Injected the `ConfigService` so we can access the env variables
- Created a new instance of the `Storage` class, passing in the relevant credentials. The private `storage` property is now an instance of the `Storage` class, which we can use to interact with Google Cloud Storage

It's also worth noting that you'll need to update your bucket name to what you called it when setting it up.

Let's now add an `uploadFile` method to upload a file to Google Cloud Storage:

```ts
import { Storage } from '@google-cloud/storage'
import { Injectable } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'

@Injectable()
export class GoogleCloudService {
  private storage: Storage
  private bucket: string
  constructor(private readonly configService: ConfigService) {
    this.storage = new Storage({
      projectId: this.configService.getOrThrow(`googleCloud.projectId`),
      credentials: {
        client_email: this.configService.getOrThrow(`googleCloud.clientEmail`),
        private_key: this.configService.getOrThrow(`googleCloud.privateKey`),
      },
    })

    this.bucket = `bucketname` // <-- add your bucket name here
  }

  async uploadFile(fileBuffer: Buffer) {
    const bucket = this.storage.bucket(this.bucket)
    const file = bucket.file(`randomfilename.jpg`) // TODO: generate random filename
    await file.save(fileBuffer)
    await file.makePublic()
    return file.publicUrl()
  }
}
```

In the `uploadFile` method, we're interacting with the GCS bucket to save a file and make it public. We're then returning the public url of the file.

You'll notice there's a TODO in the method - we need to generate a random filename for the file. This is because Google Cloud Storage requires a unique filename for each file. We also need to set the file content type to the mime type of the file - this is so that when we retrieve the file, the client knows how to render it.

For generating the unique filename, let's create a new service in the `/utilities` directory for generating UIDs, as this functionality might be useful in other places in the application.

Let's first install a package to generate UIDs:

```bash
pnpm add uid
```

Then in the `utilities` directory, add a new `uid` service

```bash
npx nest generate service utilities/uid
```

In the `uid.service.ts` file, add the following:

```ts
import { Injectable } from '@nestjs/common'
import { uid } from 'uid'

@Injectable()
export class UidService {
  generate(length?: number) {
    return uid(length)
  }
}
```

Make sure you update the `UtilitiesModule` to include the `UidService`:

```ts
import { Module } from '@nestjs/common'
import { FileService } from './file/file.service'
import { UidService } from './uid/uid.service'

@Module({
  providers: [FileService, UidService],
  exports: [FileService, UidService],
})
export class UtilitiesModule {}
```

Great! Let's now continue working on the `GoogleCloudService` to generate a random filename and set the content type.

First update the `GoogleCloudModule` to import the `UtilitiesModule`:

```ts
import { Module } from '@nestjs/common'
import { GoogleCloudService } from './google-cloud.service'
import { UtilitiesModule } from '../../utilities/utilities.module'

@Module({
  imports: [UtilitiesModule], // <-- add this
  providers: [GoogleCloudService],
  exports: [GoogleCloudService],
})
export class GoogleCloudModule {}
```

Then let's import the `UidService` into the `GoogleCloudService` and use it to generate a unique filename:

```ts
import { Storage } from '@google-cloud/storage'
import { Injectable } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'
import { UidService } from '../../utilities/uid/uid.service'

@Injectable()
export class GoogleCloudService {
  private storage: Storage
  private bucket: string
  constructor(
    private readonly configService: ConfigService,
    private readonly uidService: UidService
  ) {
    this.storage = new Storage({
      projectId: this.configService.getOrThrow(`googleCloud.projectId`),
      credentials: {
        client_email: this.configService.getOrThrow(`googleCloud.clientEmail`),
        private_key: this.configService.getOrThrow(`googleCloud.privateKey`),
      },
    })

    this.bucket = `listings-83490834`
  }

  async uploadFile(fileBuffer: Buffer, fileMimeType: string) {
    const bucket = this.storage.bucket(this.bucket)
    const fileName = this.uidService.generate()
    const file = bucket.file(`${fileName}`)
    await file.save(fileBuffer, {
      metadata: {
        contentType: fileMimeType,
      },
    })
    await file.makePublic()
    return file.publicUrl()
  }
}
```

I've also passed in the `fileMimeType` as an argument and set this as the content type of the file when saving.

Let's go back to the consumer and use this service to upload the file to Google Cloud Storage:

```ts
import { OnQueueFailed, Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'
import { Logger } from '../../../core/logger/logger.service'
import { BaseConsumer } from '../../../core/queue/base.consumer'
import { FileService } from '../../../utilities/file/file.service'
import { GoogleCloudService } from '../../../services/google-cloud/google-cloud.service'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(
    logger: Logger,
    private readonly fileService: FileService,
    private readonly googleCloud: GoogleCloudService
  ) {
    super(logger)
  }

  @Process(`createListingImage`)
  async uploadListingImage(job: Job<UploadListingImageDto>) {
    const { base64File, mimeType } = job.data
    const buffer = this.fileService.base64ToBuffer(base64File)
    const publicUrl = await this.googleCloud.uploadFile(buffer, mimeType)
    console.log(`publicUrl:`, publicUrl)
    // TODO: save publicUrl to database
    return job.data
  }

  // ...
}
```

The final step is to save the public url to the database. Let's first add the new model and relation in the database by editing the Prisma schema:

```json
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Listing {
  id              Int       @id @default(autoincrement())
  label           String
  addressLine1    String
  addressLine2    String?
  addressCity     String
  addressZipcode  String
  addressState    String
  price           Int
  bathrooms       Int
  bedrooms        Int
  squareMeters    Int
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
  images          ListingImage[]
}

model ListingImage {
  id          Int       @id @default(autoincrement())
  url         String
  createdAt   DateTime  @default(now())
  updatedAt   DateTime  @updatedAt
  listing     Listing @relation(fields: [listingId], references: [id])
  listingId   Int
}
```

To apply these changes, run and give the migration a name:

```bash
pnpm db:migrate:dev
```

Now back in the consumer, let's use the `DatabaseService` to save the public url to the database in the `ListingImage` table:

```ts
import { OnQueueFailed, Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'
import { Logger } from '../../../core/logger/logger.service'
import { BaseConsumer } from '../../../core/queue/base.consumer'
import { FileService } from '../../../utilities/file/file.service'
import { GoogleCloudService } from '../../../services/google-cloud/google-cloud.service'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'
import { DatabaseService } from '../../../database/database.service'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(
    logger: Logger,
    private readonly fileService: FileService,
    private readonly googleCloud: GoogleCloudService,
    private readonly databaseService: DatabaseService
  ) {
    super(logger)
  }

  @Process(`createListingImage`)
  async uploadListingImage(job: Job<UploadListingImageDto>) {
    const { base64File, mimeType } = job.data
    const buffer = this.fileService.base64ToBuffer(base64File)
    const publicUrl = await this.googleCloud.uploadFile(buffer, mimeType)
    return await this.databaseService.listingImage.create({
      data: {
        listingId: job.data.listingId,
        url: publicUrl,
      },
    })
  }

  // ...
}
```

And that's it! Great work.

Before going onto testing, let's consider a refactor: imagine we want to add a new POST request to our API for someone to be able to upload an individual image to a listing. We want to return the image url immediately (i.e. don't use a job to process the image).

We just added all this logic inside the consumer - so how can we reusue this logic?

Well, you know in NestJS it's best practice to keep your controllers as thin as possible with all the logic in the services?

It's often a good idea to do the same thing with your consumers - keep them as thin as possible and move the logic into the services.

This way, the service layer can be leveraged in multiple places.

Let's add a new method to the `ListingService` to upload an image to a listing:

```ts
import { Injectable } from '@nestjs/common'
import { CreateListingDto } from './dto/create-listing.dto'
import { DatabaseService } from '../../database/database.service'
import { ListingProducer } from './queue/listing.producer'
import { FileService } from '../../utilities/file/file.service'
import { UploadListingImageDto } from './dto/upload-listing-image.dto'
import { GoogleCloudService } from '../../services/google-cloud/google-cloud.service'

@Injectable()
export class ListingService {
  constructor(
    private readonly databaseService: DatabaseService,
    private readonly listingQueue: ListingProducer,
    private readonly fileService: FileService,
    private readonly googleCloud: GoogleCloudService
  ) {}

  // ...

  async uploadImage(data: UploadListingImageDto) {
    const { base64File, mimeType, listingId } = data
    const buffer = this.fileService.base64ToBuffer(base64File)
    const publicUrl = await this.googleCloud.uploadFile(buffer, mimeType)
    return await this.databaseService.listingImage.create({
      data: {
        listingId: listingId,
        url: publicUrl,
      },
    })
  }
}
```

Then you can refactor the consumer to use this method:

```ts
import { OnQueueFailed, Process, Processor } from '@nestjs/bull'
import { Job } from 'bull'
import { LISTING_QUEUE } from '../../../core/queue/queue.constants'
import { Logger } from '../../../core/logger/logger.service'
import { BaseConsumer } from '../../../core/queue/base.consumer'
import { UploadListingImageDto } from '../dto/upload-listing-image.dto'
import { ListingService } from '../listing.service'

@Processor(LISTING_QUEUE)
export class ListingConsumer extends BaseConsumer {
  constructor(logger: Logger, private readonly listingService: ListingService) {
    super(logger)
  }

  @Process(`createListingImage`)
  async uploadListingImage(job: Job<UploadListingImageDto>) {
    return await this.listingService.uploadImage(job.data)
  }

  // ...
}
```

As you can see, keeping all the logic in the service layer is a good idea as it allows you to reuse the logic in multiple places.

## Testing

In order to upload files to Google Cloud Storage, you'll need:

- A Google Cloud account and project ID
- A Google Cloud Storage bucket
- A service account with the relevant permissions to upload files to the bucket

- Upload buffer to GCS
- Store url in database

## Setting up Bull Arena

- remove Bull arena from http logging

```

```
